{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T01:55:48.250223Z",
     "start_time": "2017-12-20T01:55:46.809797Z"
    },
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from tensorflow.python.layers import core as core_layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import myResidualCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "import jieba\n",
    "from bleu import BLEU\n",
    "import random\n",
    "import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CVRAE:\n",
    "    def __init__(self, dp, rnn_size, n_layers, latent_dim, encoder_embedding_dim, decoder_embedding_dim, max_infer_length,\n",
    "                 sess=tf.Session(), lr=0.001, grad_clip=5.0, beam_width=5, force_teaching_ratio=1.0, beam_penalty=1.0,\n",
    "                residual=False, output_keep_prob=0.5, input_keep_prob=0.9, cell_type='lstm', reverse=False,\n",
    "                 condition_embedding_dim=4, num_c=3,\n",
    "                latent_weight=0.1, beta_decay_period=10, beta_decay_offset=5, decay_scheme='luong234', is_save=True):\n",
    "        \n",
    "        self.rnn_size = rnn_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.grad_clip = grad_clip\n",
    "        self.dp = dp\n",
    "        self.step = 0\n",
    "        self.encoder_embedding_dim = encoder_embedding_dim\n",
    "        self.decoder_embedding_dim = decoder_embedding_dim\n",
    "        self.beam_width = beam_width\n",
    "        self.num_c = num_c\n",
    "        self.condition_embedding_dim = condition_embedding_dim\n",
    "        self.latent_weight = latent_weight\n",
    "        self.beam_penalty = beam_penalty\n",
    "        self.max_infer_length = max_infer_length\n",
    "        self.residual = residual\n",
    "        self.is_save = is_save\n",
    "        self.decay_scheme = decay_scheme\n",
    "        if self.residual:\n",
    "            assert encoder_embedding_dim == rnn_size\n",
    "            assert decoder_embedding_dim == rnn_size\n",
    "        self.reverse = reverse\n",
    "        self.cell_type = cell_type\n",
    "        self.force_teaching_ratio = force_teaching_ratio\n",
    "        self._output_keep_prob = output_keep_prob\n",
    "        self._input_keep_prob = input_keep_prob\n",
    "        self.beta_decay_period = beta_decay_period\n",
    "        self.beta_decay_offset = beta_decay_offset\n",
    "        self.sess = sess\n",
    "        self.lr=lr\n",
    "        self.build_graph()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep = 10)\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n",
    "        \n",
    "    # end constructor\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.register_symbols()\n",
    "        self.add_input_layer()\n",
    "        self.add_encoder_layer()\n",
    "        self.add_stochastic_layer()\n",
    "        self.add_decoder_hidden()\n",
    "        with tf.variable_scope('decode'):\n",
    "            self.add_decoder_for_training()\n",
    "        with tf.variable_scope('decode', reuse=True):\n",
    "            self.add_decoder_for_inference()\n",
    "        with tf.variable_scope('decode', reuse=True):\n",
    "            self.add_decoder_for_prefix_inference()\n",
    "        self.add_backward_path()\n",
    "    # end method\n",
    "\n",
    "    def add_input_layer(self):\n",
    "        self.X = tf.placeholder(tf.int32, [None, None], name=\"X\")\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None], name=\"Y\")\n",
    "        self.C = tf.placeholder(tf.int32, [None], name='Condition')\n",
    "        self.X_seq_len = tf.placeholder(tf.int32, [None], name=\"X_seq_len\")\n",
    "        self.Y_seq_len = tf.placeholder(tf.int32, [None], name=\"Y_seq_len\")\n",
    "        self.input_keep_prob = tf.placeholder(tf.float32,name=\"input_keep_prob\")\n",
    "        self.output_keep_prob = tf.placeholder(tf.float32,name=\"output_keep_prob\")\n",
    "        self.batch_size = tf.shape(self.X)[0]\n",
    "        self.B = tf.placeholder(tf.float32, name='Beta_deterministic_warmup')\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    # end method\n",
    "\n",
    "    def single_cell(self, reuse=False):\n",
    "        if self.cell_type == 'lstm':\n",
    "             cell = tf.contrib.rnn.LayerNormBasicLSTMCell(self.rnn_size, reuse=reuse)\n",
    "        else:\n",
    "            cell = tf.contrib.rnn.GRUBlockCell(self.rnn_size)    \n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, self.output_keep_prob, self.input_keep_prob)\n",
    "        if self.residual:\n",
    "            cell = myResidualCell.ResidualWrapper(cell)\n",
    "        return cell\n",
    "    \n",
    "    def add_encoder_layer(self):\n",
    "        encoder_embedding = tf.get_variable('encoder_embedding', [len(self.dp.X_w2id), self.encoder_embedding_dim],\n",
    "                                             tf.float32, tf.random_uniform_initializer(-1.0, 1.0))\n",
    "        self.c_embedding = tf.get_variable('condition_embedding', [self.num_c, self.condition_embedding_dim],\n",
    "                                             tf.float32, tf.random_uniform_initializer(-1.0, 1.0))\n",
    "        self.c_inputs = tf.nn.embedding_lookup(self.c_embedding, self.C)\n",
    "        self.encoder_inputs = tf.nn.embedding_lookup(encoder_embedding, self.X)\n",
    "        bi_encoder_output, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw = tf.contrib.rnn.MultiRNNCell([self.single_cell() for _ in range(self.n_layers)]), \n",
    "            cell_bw = tf.contrib.rnn.MultiRNNCell([self.single_cell() for _ in range(self.n_layers)]),\n",
    "            inputs = self.encoder_inputs,\n",
    "            sequence_length = self.X_seq_len,\n",
    "            dtype = tf.float32,\n",
    "            scope = 'bidirectional_rnn')\n",
    "        #print bi_encoder_state\n",
    "        if self.cell_type == 'lstm':\n",
    "            self.encoder_out = tf.concat([bi_encoder_state[0][-1][1],bi_encoder_state[1][-1][1]], -1)\n",
    "        else:\n",
    "            self.encoder_out = tf.concat([bi_encoder_state[0][-1],bi_encoder_state[1][-1]], -1)\n",
    "        \n",
    "    def add_stochastic_layer(self):\n",
    "        # reparametrization trick\n",
    "        self.z_mu = tf.layers.dense(self.encoder_out, self.latent_dim)\n",
    "        self.z_lgs2 = tf.layers.dense(self.encoder_out, self.latent_dim)\n",
    "        noise = tf.random_normal(tf.shape(self.z_lgs2))\n",
    "        self._z = self.z_mu + tf.exp(0.5 * self.z_lgs2) * noise\n",
    "        self.z = tf.concat([self._z, self.c_inputs], -1)\n",
    "        \n",
    "    def add_decoder_hidden(self):\n",
    "        hidden_state_list = []\n",
    "        for i in range(self.n_layers * 2):\n",
    "            if self.cell_type == 'gru':\n",
    "                hidden_state_list.append(tf.layers.dense(self.z, self.rnn_size))\n",
    "            else:\n",
    "                hidden_state_list.append(tf.contrib.rnn.LSTMStateTuple(tf.layers.dense(self.z, self.rnn_size), tf.layers.dense(self.z, self.rnn_size))) \n",
    "        self.decoder_init_state = tuple(hidden_state_list)\n",
    "        \n",
    "    def processed_decoder_input(self):\n",
    "        main = tf.strided_slice(self.Y, [0, 0], [self.batch_size, -1], [1, 1]) # remove last char\n",
    "        decoder_input = tf.concat([tf.fill([self.batch_size, 1], self._y_go), main], 1)\n",
    "        return decoder_input\n",
    "\n",
    "    def add_decoder_for_training(self):\n",
    "        self.decoder_cell = tf.contrib.rnn.MultiRNNCell([self.single_cell() for _ in range(2 * self.n_layers)])\n",
    "        decoder_embedding = tf.get_variable('decoder_embedding', [len(self.dp.Y_w2id), self.decoder_embedding_dim],\n",
    "                                             tf.float32, tf.random_uniform_initializer(-1.0, 1.0))\n",
    "        emb = tf.nn.embedding_lookup(decoder_embedding, self.processed_decoder_input())\n",
    "        inputs = tf.expand_dims(self.z, 1)\n",
    "        inputs = tf.tile(inputs, [1, tf.shape(emb)[1], 1])\n",
    "        inputs = tf.concat([emb, inputs],2) \n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            inputs = inputs,\n",
    "            sequence_length = self.Y_seq_len,\n",
    "            time_major = False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell = self.decoder_cell,\n",
    "            helper = training_helper,\n",
    "            initial_state = self.decoder_init_state, #self.decoder_cell.zero_state(self.batch_size, tf.float32),\n",
    "            output_layer = core_layers.Dense(len(self.dp.Y_w2id)))\n",
    "        training_decoder_output, training_final_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder = training_decoder,\n",
    "            impute_finished = True,\n",
    "            maximum_iterations = tf.reduce_max(self.Y_seq_len))\n",
    "        self.training_logits = training_decoder_output.rnn_output\n",
    "        self.init_prefix_state = training_final_state\n",
    "\n",
    "    def add_decoder_for_inference(self):   \n",
    "        decoder_embedding = tf.get_variable('decoder_embedding')\n",
    "        self.beam_f = (lambda ids: tf.concat([tf.nn.embedding_lookup(decoder_embedding, ids), \n",
    "                                    tf.tile(tf.expand_dims(self.z, 1), \n",
    "                                            [1,int(tf.nn.embedding_lookup(decoder_embedding, ids).get_shape()[1]), 1]) if len(ids.get_shape()) !=1 \n",
    "                                             else self.z], -1))\n",
    "\n",
    "        predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "            cell = self.decoder_cell,\n",
    "            embedding = self.beam_f, \n",
    "            start_tokens = tf.tile(tf.constant([self._y_go], dtype=tf.int32), [self.batch_size]),\n",
    "            end_token = self._y_eos,\n",
    "            initial_state = tf.contrib.seq2seq.tile_batch(self.decoder_init_state, self.beam_width),#self.decoder_cell.zero_state(self.batch_size * self.beam_width, tf.float32),\n",
    "            beam_width = self.beam_width,\n",
    "            output_layer = core_layers.Dense(len(self.dp.Y_w2id), _reuse=True),\n",
    "            length_penalty_weight = self.beam_penalty)\n",
    "        predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder = predicting_decoder,\n",
    "            impute_finished = False,\n",
    "            maximum_iterations = self.max_infer_length)\n",
    "        self.predicting_ids = predicting_decoder_output.predicted_ids\n",
    "        self.score = predicting_decoder_output.beam_search_decoder_output.scores\n",
    "        \n",
    "    def add_decoder_for_prefix_inference(self):   \n",
    "        predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "            cell = self.decoder_cell,\n",
    "            embedding = self.beam_f,\n",
    "            start_tokens = tf.tile(tf.constant([self._y_go], dtype=tf.int32), [self.batch_size]),\n",
    "            end_token = self._y_eos,\n",
    "            initial_state = tf.contrib.seq2seq.tile_batch(self.init_prefix_state, self.beam_width),\n",
    "            beam_width = self.beam_width,\n",
    "            output_layer = core_layers.Dense(len(self.dp.Y_w2id), _reuse=True),\n",
    "            length_penalty_weight = self.beam_penalty)\n",
    "        \n",
    "        self.prefix_go = tf.placeholder(tf.int32, [None])\n",
    "        prefix_go_beam = tf.tile(tf.expand_dims(self.prefix_go, 1), [1, self.beam_width])\n",
    "        prefix_emb = self.beam_f(prefix_go_beam)\n",
    "        predicting_decoder._start_inputs = prefix_emb\n",
    "        predicting_prefix_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder = predicting_decoder,\n",
    "            impute_finished = False,\n",
    "            maximum_iterations = self.max_infer_length)\n",
    "        self.predicting_prefix_ids = predicting_prefix_decoder_output.predicted_ids\n",
    "        self.prefix_score = predicting_prefix_decoder_output.beam_search_decoder_output.scores\n",
    "\n",
    "    def add_backward_path(self):\n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        self.reconstruct_loss = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
    "                                                     targets = self.Y,\n",
    "                                                     weights = masks)\n",
    "        self.batch_reconstruct_loss = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
    "                                                     targets = self.Y,\n",
    "                                                     weights = masks,\n",
    "                                                     average_across_batch=False)\n",
    "        self.kl_loss = tf.reduce_mean(-0.5 * tf.reduce_sum(1 + self.z_lgs2 - tf.square(self.z_mu) - tf.exp(self.z_lgs2), 1))\n",
    "        self.loss = self.reconstruct_loss + self.B * self.latent_weight * self.kl_loss\n",
    "        \n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(self.loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.grad_clip)\n",
    "        self.learning_rate = tf.constant(self.lr)\n",
    "        self.learning_rate = self.get_learning_rate_decay(self.decay_scheme)  # decay\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n",
    "\n",
    "    def register_symbols(self):\n",
    "        self._x_go = self.dp.X_w2id['<GO>']\n",
    "        self._x_eos = self.dp.X_w2id['<EOS>']\n",
    "        self._x_pad = self.dp.X_w2id['<PAD>']\n",
    "        self._x_unk = self.dp.X_w2id['<UNK>']\n",
    "        \n",
    "        self._y_go = self.dp.Y_w2id['<GO>']\n",
    "        self._y_eos = self.dp.Y_w2id['<EOS>']\n",
    "        self._y_pad = self.dp.Y_w2id['<PAD>']\n",
    "        self._y_unk = self.dp.Y_w2id['<UNK>']\n",
    "    \n",
    "    def infer(self, input_word, c):\n",
    "        input_word = list(jieba.cut(input_word))\n",
    "        if self.reverse:\n",
    "            input_word = input_word[::-1]\n",
    "        input_indices = [self.dp.X_w2id.get(char, self._x_unk) for char in input_word]\n",
    "        out_indices = self.sess.run(self.predicting_ids, {\n",
    "            self.X: [input_indices], self.C:[c], self.X_seq_len: [len(input_indices)], self.output_keep_prob:1, self.input_keep_prob:1})\n",
    "        outputs = []\n",
    "        for idx in range(out_indices.shape[-1]):\n",
    "            eos_id = self.dp.Y_w2id['<EOS>']\n",
    "            ot = out_indices[0,:,idx]\n",
    "            if eos_id in ot:\n",
    "                ot = ot.tolist()\n",
    "                ot = ot[:ot.index(eos_id)]\n",
    "            if self.reverse:\n",
    "                ot = ot[::-1]\n",
    "            output_str = ''.join([self.dp.Y_id2w.get(i, u'&') for i in ot])\n",
    "            outputs.append(output_str)\n",
    "        return outputs\n",
    "    \n",
    "    def prefix_infer(self, input_word, prefix, c):\n",
    "        input_word = list(jieba.cut(input_word))\n",
    "        prefix = list(jieba.cut(prefix))\n",
    "        input_indices_X = [self.dp.X_w2id.get(char, self._x_unk) for char in input_word]\n",
    "        input_indices_Y = [self.dp.Y_w2id.get(char, self._y_unk) for char in prefix]\n",
    "        \n",
    "        prefix_go = []\n",
    "        prefix_go.append(input_indices_Y[-1]) \n",
    "        out_indices, scores = self.sess.run([self.predicting_prefix_ids, self.prefix_score], {\n",
    "            self.X: [input_indices_X], self.X_seq_len: [len(input_indices_X)], self.C:[c], self.Y:[input_indices_Y], self.Y_seq_len:[len(input_indices_Y)],\n",
    "            self.prefix_go: prefix_go, self.input_keep_prob:1, self.output_keep_prob:1})\n",
    "        \n",
    "        outputs = []\n",
    "        for idx in range(out_indices.shape[-1]):\n",
    "            eos_id = self.dp.Y_w2id['<EOS>']\n",
    "            ot = out_indices[0,:,idx]\n",
    "            if eos_id in ot:\n",
    "                ot = ot.tolist()\n",
    "                ot = ot[:ot.index(eos_id)]\n",
    "                if self.reverse:\n",
    "                    ot = ot[::-1]\n",
    "            if self.reverse:\n",
    "                output_str = ''.join([self.dp.Y_id2w.get(i, u'&') for i in ot]) + prefix\n",
    "            else:\n",
    "                output_str = prefix + ''.join([self.dp.Y_id2w.get(i, u'&') for i in ot])\n",
    "            outputs.append(output_str)\n",
    "        return outputs\n",
    "    \n",
    "    def xToz(self, input_word):\n",
    "        input_word = list(jieba.cut(input_word))\n",
    "        if self.reverse:\n",
    "            input_word = input_word[::-1]\n",
    "        input_indices = [self.dp.X_w2id.get(char, self._x_unk) for char in input_word]\n",
    "        z = self.sess.run(self._z, {self.X: [input_indices], self.X_seq_len: [len(input_indices)], self.output_keep_prob:1, self.input_keep_prob:1})\n",
    "        return z\n",
    "    # end method\n",
    "    \n",
    "    def zTox(self, z, c):\n",
    "        out_indices = self.sess.run(self.predicting_ids, {self.batch_size:z.shape[0],\n",
    "            self._z:z, self.C:[c], self.output_keep_prob:1, self.input_keep_prob:1})\n",
    "        outputs = []\n",
    "        for idx in range(out_indices.shape[-1]):\n",
    "            eos_id = self.dp.Y_w2id['<EOS>']\n",
    "            ot = out_indices[0,:,idx]\n",
    "            if eos_id in ot:\n",
    "                ot = ot.tolist()\n",
    "                ot = ot[:ot.index(eos_id)]\n",
    "            if self.reverse:\n",
    "                ot = ot[::-1]\n",
    "            output_str = ''.join([self.dp.Y_id2w.get(i, u'&') for i in ot])\n",
    "            outputs.append(output_str)\n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, c, batch_size = 3):\n",
    "        c_batch = [c for _ in range(batch_size)]\n",
    "        out_indices = self.sess.run(self.predicting_ids, { self.batch_size:batch_size,\n",
    "            self._z:np.random.randn(batch_size, self.latent_dim), self.C: c_batch, self.output_keep_prob:1, self.input_keep_prob:1})\n",
    "        outputs = []\n",
    "        for idx in range(out_indices.shape[0]):\n",
    "            eos_id = self.dp.Y_w2id['<EOS>']\n",
    "            ot = out_indices[idx,:,0]   # The 0th beam of each batch \n",
    "            if eos_id in ot:\n",
    "                ot = ot.tolist()\n",
    "                ot = ot[:ot.index(eos_id)]\n",
    "            if self.reverse:\n",
    "                ot = ot[::-1]\n",
    "            output_str = ''.join([self.dp.Y_id2w.get(i, u'&') for i in ot])\n",
    "            outputs.append(output_str)\n",
    "        return outputs\n",
    "    \n",
    "    def restore(self, path):\n",
    "        self.saver.restore(self.sess, path)\n",
    "        print 'restore %s success' % path\n",
    "        \n",
    "    def get_learning_rate_decay(self, decay_scheme='luong234'):\n",
    "        num_train_steps = self.dp.num_steps\n",
    "        if decay_scheme == \"luong10\":\n",
    "            start_decay_step = int(num_train_steps / 2)\n",
    "            remain_steps = num_train_steps - start_decay_step\n",
    "            decay_steps = int(remain_steps / 10)  # decay 10 times\n",
    "            decay_factor = 0.5\n",
    "        else:\n",
    "            start_decay_step = int(num_train_steps * 2 / 3)\n",
    "            remain_steps = num_train_steps - start_decay_step\n",
    "            decay_steps = int(remain_steps / 4)  # decay 4 times\n",
    "            decay_factor = 0.5\n",
    "        return tf.cond(\n",
    "            self.global_step < start_decay_step,\n",
    "            lambda: self.learning_rate,\n",
    "            lambda: tf.train.exponential_decay(\n",
    "                self.learning_rate,\n",
    "                (self.global_step - start_decay_step),\n",
    "                decay_steps, decay_factor, staircase=True),\n",
    "            name=\"learning_rate_decay_cond\")\n",
    "    \n",
    "    def setup_summary(self):\n",
    "        train_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar('Train_loss', train_loss) \n",
    "        train_KL_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar('Train_KL_loss', train_KL_loss)\n",
    "        train_r_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar('Train_R_loss', train_r_loss)\n",
    "        test_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar('Test_loss', test_loss) \n",
    "        test_KL_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar('Test_KL_loss', test_KL_loss)\n",
    "        test_r_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar('Test_R_loss', test_r_loss)\n",
    "        beta = tf.Variable(0.)\n",
    "        tf.summary.scalar('Beta', beta)\n",
    "        tf.summary.scalar('lr_rate', self.learning_rate)\n",
    "        tf.summary.histogram(\"z_mu\", self.z_mu)\n",
    "        tf.summary.histogram(\"z_ls2\", self.z_lgs2)\n",
    "        tf.summary.histogram(\"z\", self.z)\n",
    "        \n",
    "        summary_vars = [train_loss, train_KL_loss, train_r_loss, test_loss, test_KL_loss, test_r_loss, beta]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in xrange(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in xrange(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T01:55:51.813493Z",
     "start_time": "2017-12-20T01:55:51.736246Z"
    },
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class CVRAE_DP:\n",
    "    def __init__(self, X_indices, Y_indices, C, X_w2id, Y_w2id, BATCH_SIZE, n_epoch):\n",
    "        assert len(X_indices) == len(Y_indices)\n",
    "        num_test = int(len(X_indices) * 0.1)\n",
    "        self.n_epoch = n_epoch\n",
    "        self.X_train = np.array(X_indices[num_test:])\n",
    "        self.Y_train = np.array(Y_indices[num_test:])\n",
    "        self.C_train = np.array(C[num_test:])\n",
    "        self.X_test = np.array(X_indices[:num_test])\n",
    "        self.Y_test = np.array(Y_indices[:num_test])\n",
    "        self.C_test = np.array(C[num_test:])\n",
    "        self.num_batch = int(len(self.X_train) / BATCH_SIZE)\n",
    "        self.num_steps = self.num_batch * self.n_epoch\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.X_w2id = X_w2id\n",
    "        self.X_id2w = dict(zip(X_w2id.values(), X_w2id.keys()))\n",
    "        self.Y_w2id = Y_w2id\n",
    "        self.Y_id2w = dict(zip(Y_w2id.values(), Y_w2id.keys()))\n",
    "        self._x_pad = self.X_w2id['<PAD>']\n",
    "        self._y_pad = self.Y_w2id['<PAD>']\n",
    "        print 'Train_data: %d | Test_data: %d | Batch_size: %d | Num_batch: %d | X_vocab_size: %d | Y_vocab_size: %d' % (len(self.X_train), len(self.X_test), BATCH_SIZE, self.num_batch, len(self.X_w2id), len(self.Y_w2id))\n",
    "        \n",
    "    def next_batch(self, X, Y, C):\n",
    "        r = np.random.permutation(len(X))\n",
    "        X = X[r]\n",
    "        Y = Y[r]\n",
    "        C = C[r]\n",
    "        for i in range(0, len(X) - len(X) % self.batch_size, self.batch_size):\n",
    "            X_batch = X[i : i + self.batch_size]\n",
    "            Y_batch = Y[i : i + self.batch_size]\n",
    "            c_batch = C[i : i + self.batch_size]\n",
    "            padded_X_batch, X_batch_lens = self.pad_sentence_batch(X_batch, self._x_pad)\n",
    "            padded_Y_batch, Y_batch_lens = self.pad_sentence_batch(Y_batch, self._y_pad)\n",
    "            yield (np.array(padded_X_batch),\n",
    "                   np.array(padded_Y_batch),\n",
    "                   c_batch,\n",
    "                   X_batch_lens,\n",
    "                   Y_batch_lens)\n",
    "    \n",
    "    def sample_test_batch(self):\n",
    "        padded_X_batch, X_batch_lens = self.pad_sentence_batch(self.X_test[: self.batch_size], self._x_pad)\n",
    "        padded_Y_batch, Y_batch_lens = self.pad_sentence_batch(self.Y_test[: self.batch_size], self._y_pad)\n",
    "        c_batch = self.C_test[: self.batch_size]\n",
    "        return np.array(padded_X_batch), np.array(padded_Y_batch), c_batch, X_batch_lens, Y_batch_lens\n",
    "        \n",
    "    def pad_sentence_batch(self, sentence_batch, pad_int):\n",
    "        padded_seqs = []\n",
    "        seq_lens = []\n",
    "        max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "        for sentence in sentence_batch:\n",
    "            padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "            seq_lens.append(len(sentence))\n",
    "        return padded_seqs, seq_lens\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T01:55:53.227402Z",
     "start_time": "2017-12-20T01:55:52.571240Z"
    },
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import scipy.interpolate as si\n",
    "from scipy import interpolate\n",
    "\n",
    "\n",
    "def BetaGenerator(epoches, beta_decay_period, beta_decay_offset):\n",
    "    points = [[0,0], [0, beta_decay_offset],[0, beta_decay_offset + 0.33 * beta_decay_period], [1, beta_decay_offset + 0.66*beta_decay_period],[1, beta_decay_offset + beta_decay_period], [1, epoches] ];\n",
    "    points = np.array(points)\n",
    "    x = points[:,0]\n",
    "    y = points[:,1]\n",
    "    t = range(len(points))\n",
    "    ipl_t = np.linspace(0.0, len(points) - 1, 100)\n",
    "    x_tup = si.splrep(t, x, k=3)\n",
    "    y_tup = si.splrep(t, y, k=3)\n",
    "    x_list = list(x_tup)\n",
    "    xl = x.tolist()\n",
    "    x_list[1] = xl + [0.0, 0.0, 0.0, 0.0]\n",
    "    y_list = list(y_tup)\n",
    "    yl = y.tolist()\n",
    "    y_list[1] = yl + [0.0, 0.0, 0.0, 0.0]\n",
    "    x_i = si.splev(ipl_t, x_list)\n",
    "    y_i = si.splev(ipl_t, y_list)\n",
    "    return interpolate.interp1d(y_i, x_i)\n",
    "\n",
    "class CVRAE_util:\n",
    "    def __init__(self, dp, model, display_freq=3):\n",
    "        self.display_freq = display_freq\n",
    "        self.dp = dp\n",
    "        self.model = model\n",
    "        self.summary_cnt = 0\n",
    "        self.betaG = BetaGenerator(self.dp.n_epoch*self.dp.num_batch, self.model.beta_decay_period*self.dp.num_batch, self.model.beta_decay_offset*self.dp.num_batch)\n",
    "        \n",
    "    def train(self, epoch):\n",
    "        avg_loss = 0.0\n",
    "        avg_r_loss = 0.0\n",
    "        avg_kl_loss = 0.0\n",
    "        tic = time.time()\n",
    "        X_test_batch, Y_test_batch, C_test_batch, X_test_batch_lens, Y_test_batch_lens = self.dp.sample_test_batch()\n",
    "        for local_step, (X_train_batch, Y_train_batch, C_train_batch, X_train_batch_lens, Y_train_batch_lens) in enumerate(\n",
    "            self.dp.next_batch(self.dp.X_train, self.dp.Y_train, self.dp.C_train)):\n",
    "            beta = 0.001 + self.betaG(self.model.step) # add small value to avoid points to scatter\n",
    "            self.model.step, _, loss, r_loss, kl_loss = self.model.sess.run([self.model.global_step, self.model.train_op, \n",
    "                                                            self.model.loss, self.model.reconstruct_loss, self.model.kl_loss], \n",
    "                                          {self.model.X: X_train_batch,\n",
    "                                           self.model.Y: Y_train_batch,\n",
    "                                           self.model.C: C_train_batch,\n",
    "                                           self.model.X_seq_len: X_train_batch_lens,\n",
    "                                           self.model.Y_seq_len: Y_train_batch_lens,\n",
    "                                           self.model.output_keep_prob:self.model._output_keep_prob,\n",
    "                                           self.model.input_keep_prob:self.model._input_keep_prob,\n",
    "                                          self.model.B:beta})\n",
    "            avg_loss += loss\n",
    "            avg_r_loss += r_loss\n",
    "            avg_kl_loss += kl_loss\n",
    "            # summary\n",
    "            if local_step % 10 == 0:\n",
    "                self.summary_cnt += 1\n",
    "                val_loss, val_r_loss, val_kl_loss = self.model.sess.run([self.model.loss, self.model.reconstruct_loss, self.model.kl_loss], \n",
    "                                               {self.model.X: X_test_batch,\n",
    "                                                     self.model.Y: Y_test_batch,\n",
    "                                                     self.model.C: C_test_batch,\n",
    "                                                     self.model.X_seq_len: X_test_batch_lens,\n",
    "                                                     self.model.Y_seq_len: Y_test_batch_lens,\n",
    "                                                     self.model.output_keep_prob:1,\n",
    "                                                     self.model.input_keep_prob:1,\n",
    "                                                     self.model.B:beta})\n",
    "                stats = [avg_loss/(local_step+1), avg_kl_loss/(local_step+1), avg_r_loss/(local_step+1),\n",
    "                         val_loss, val_kl_loss, val_r_loss, beta]\n",
    "                for i in xrange(len(stats)):\n",
    "                    self.model.sess.run(self.model.update_ops[i], feed_dict={\n",
    "                        self.model.summary_placeholders[i]: float(stats[i])\n",
    "                    })\n",
    "                summary_str = self.model.sess.run(self.model.summary_op, {\n",
    "                    self.model.X: X_test_batch, \n",
    "                    self.model.X_seq_len: X_test_batch_lens,\n",
    "                    self.model.C: C_test_batch,\n",
    "                    self.model.output_keep_prob:1,\n",
    "                    self.model.input_keep_prob:1})\n",
    "                self.summary_writer.add_summary(summary_str, self.summary_cnt)\n",
    "                \n",
    "            if local_step % (self.dp.num_batch / self.display_freq) == 0:\n",
    "                val_loss, val_r_loss, val_kl_loss = self.model.sess.run([self.model.loss, self.model.reconstruct_loss, self.model.kl_loss], \n",
    "                                               {self.model.X: X_test_batch,\n",
    "                                                     self.model.Y: Y_test_batch,\n",
    "                                                     self.model.C: C_test_batch,\n",
    "                                                     self.model.X_seq_len: X_test_batch_lens,\n",
    "                                                     self.model.Y_seq_len: Y_test_batch_lens,\n",
    "                                                     self.model.output_keep_prob:1,\n",
    "                                                     self.model.input_keep_prob:1,\n",
    "                                                     self.model.B:beta})\n",
    "                print \"Epoch %d/%d | Batch %d/%d | Train_loss: %.3f = %.3f + %.3f | Test_loss: %.3f = %.3f + %.3f | Time_cost:%.3f\" % (epoch, self.n_epoch, local_step, self.dp.num_batch, \n",
    "                                                                                                                                       avg_loss / (local_step + 1),\n",
    "                                                                                                                                       avg_r_loss / (local_step + 1),\n",
    "                                                                                                                                       avg_kl_loss / (local_step + 1),\n",
    "                                                                                                                                       val_loss, val_r_loss, val_kl_loss, time.time()-tic)\n",
    "                self.cal()\n",
    "                tic = time.time()\n",
    "        return avg_loss / self.dp.num_batch, avg_r_loss / self.dp.num_batch, avg_kl_loss / self.dp.num_batch\n",
    "    \n",
    "    def test(self):\n",
    "        avg_loss = 0.0\n",
    "        avg_r_loss = 0.0\n",
    "        avg_kl_loss = 0.0\n",
    "        beta = 0.001 + self.betaG(self.model.step) # add small value to avoid points to scatter\n",
    "        for local_step, (X_test_batch, Y_test_batch, C_test_batch, X_test_batch_lens, Y_test_batch_lens) in enumerate(\n",
    "            self.dp.next_batch(self.dp.X_test, self.dp.Y_test, self.dp.C_test)):\n",
    "            val_loss, val_r_loss, val_kl_loss = self.model.sess.run([self.model.loss, self.model.reconstruct_loss, self.model.kl_loss], \n",
    "                                                                   {self.model.X: X_test_batch,\n",
    "                                                                         self.model.Y: Y_test_batch,\n",
    "                                                                         self.model.C: C_test_batch,\n",
    "                                                                         self.model.X_seq_len: X_test_batch_lens,\n",
    "                                                                         self.model.Y_seq_len: Y_test_batch_lens,\n",
    "                                                                         self.model.output_keep_prob:1,\n",
    "                                                                         self.model.input_keep_prob:1,\n",
    "                                                                         self.model.B:beta})\n",
    "            avg_loss += val_loss\n",
    "            avg_r_loss += val_r_loss\n",
    "            avg_kl_loss += val_kl_loss\n",
    "        return avg_loss / (local_step + 1), avg_r_loss / (local_step + 1), avg_kl_loss / (local_step + 1)\n",
    "    \n",
    "    def fit(self, train_dir, is_bleu):\n",
    "        self.n_epoch = self.dp.n_epoch\n",
    "        test_loss_list = []\n",
    "        train_loss_list = []\n",
    "        test_r_loss_list = []\n",
    "        train_r_loss_list = []\n",
    "        test_kl_loss_list = []\n",
    "        train_kl_loss_list = []\n",
    "        time_cost_list = []\n",
    "        bleu_list = []\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(train_dir, \"runs\", timestamp))\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        print \"Writing to %s\" % out_dir\n",
    "        checkpoint_prefix = os.path.join(out_dir, \"model\")\n",
    "        self.summary_writer = tf.summary.FileWriter(os.path.join(out_dir, 'Summary'), self.model.sess.graph)\n",
    "        for epoch in range(1, self.n_epoch+1):\n",
    "            tic = time.time()\n",
    "            train_loss, train_r_loss, train_kl_loss = self.train(epoch)\n",
    "            train_loss_list.append(train_loss)\n",
    "            train_r_loss_list.append(train_r_loss)\n",
    "            train_kl_loss_list.append(train_kl_loss)\n",
    "            \n",
    "            test_loss, test_r_loss, test_kl_loss = self.test()\n",
    "            test_loss_list.append(test_loss)\n",
    "            test_r_loss_list.append(test_r_loss)\n",
    "            test_kl_loss_list.append(test_kl_loss)\n",
    "            toc = time.time()\n",
    "            time_cost_list.append((toc - tic))\n",
    "            if is_bleu:\n",
    "                bleu = self.test_bleu()\n",
    "                bleu_list.append(bleu)\n",
    "                print \"Epoch %d/%d | Train_loss: %.3f = %.3f + %.3f | Test_loss: %.3f = %.3f + %.3f | Bleu: %.3f\" % (epoch, self.n_epoch, train_loss, train_r_loss, train_kl_loss, test_loss, test_r_loss, test_kl_loss, bleu)\n",
    "            else:\n",
    "                bleu = 0.0\n",
    "                print \"Epoch %d/%d | Train_loss: %.3f = %.3f + %.3f | Test_loss: %.3f = %.3f + %.3f\" % (epoch, self.n_epoch, train_loss, train_r_loss, train_kl_loss, test_loss, test_r_loss, test_kl_loss)\n",
    "            if self.model.is_save:\n",
    "                cPickle.dump((train_loss_list, train_r_loss_list, train_kl_loss_list, test_loss_list, test_r_loss_list, test_kl_loss_list, time_cost_list, bleu_list), open(os.path.join(out_dir,\"res.pkl\"),'wb'))\n",
    "                path = self.model.saver.save(self.model.sess, checkpoint_prefix, global_step=epoch)\n",
    "                print \"Saved model checkpoint to %s\" % path\n",
    "    \n",
    "    def show(self, sent, id2w):\n",
    "        return \"\".join([id2w.get(idx, u'&') for idx in sent])\n",
    "    \n",
    "    def cal(self, n_example=5):\n",
    "        train_n_example = int(n_example / 2)\n",
    "        test_n_example = n_example - train_n_example\n",
    "        for _ in range(test_n_example):\n",
    "            example = self.show(self.dp.X_test[_], self.dp.X_id2w)\n",
    "            y = self.show(self.dp.Y_test[_], self.dp.Y_id2w)\n",
    "            o = self.model.infer(example, c=0)[0]\n",
    "            print 'Test_Input: %s | Output: %s | GroundTruth: %s' % (example, o, y)\n",
    "        for _ in range(train_n_example):\n",
    "            example = self.show(self.dp.X_train[_], self.dp.X_id2w)\n",
    "            y = self.show(self.dp.Y_train[_], self.dp.Y_id2w)\n",
    "            o = self.model.infer(example, c=0)[0]\n",
    "            print 'Train_Input: %s | Output: %s | GroundTruth: %s' % (example, o, y) \n",
    "        o = self.model.generate(c=0)\n",
    "        print 'generate top:'\n",
    "        for oo in o:\n",
    "            print '【',oo,'】'\n",
    "        print \"\"\n",
    "        o = self.model.generate(c=1)\n",
    "        print 'generate mid:'\n",
    "        for oo in o:\n",
    "            print '【',oo,'】'\n",
    "        print \"\"\n",
    "        o = self.model.generate(c=2)\n",
    "        print 'generate down:'\n",
    "        for oo in o:\n",
    "            print '【',oo,'】'\n",
    "        print \"\"\n",
    "        \n",
    "    def test_bleu(self, N=300, gram=4):\n",
    "        all_score = []\n",
    "        for i in range(N):\n",
    "            input_indices = self.show(self.dp.X_test[i], self.dp.X_id2w)\n",
    "            o = self.model.infer(input_indices)[0]\n",
    "            refer4bleu = [[' '.join([self.dp.Y_id2w.get(w, u'&') for w in self.dp.Y_test[i]])]]\n",
    "            candi = [' '.join(w for w in o)]\n",
    "            score = BLEU(candi, refer4bleu, gram=gram)\n",
    "            all_score.append(score)\n",
    "        return np.mean(all_score)\n",
    "    \n",
    "    def show_res(self, path):\n",
    "        res = cPickle.load(open(path))\n",
    "        plt.figure(1)\n",
    "        plt.title('The train results') \n",
    "        l1, = plt.plot(res[0], 'g')\n",
    "        l2, = plt.plot(res[1], 'r')\n",
    "        l3, = plt.plot(res[2], 'b')\n",
    "        plt.legend(handles = [l1, l2, l3], labels = [\"Train_loss\",\"Train_r_loss\",\"Train_kl_loss\"], loc = 'best')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(1)\n",
    "        plt.title('The test results') \n",
    "        l4, = plt.plot(res[3], 'g')\n",
    "        l5, = plt.plot(res[4], 'r')\n",
    "        l6, = plt.plot(res[5], 'r')\n",
    "        l7, = plt.plot(res[-1], 'b')\n",
    "        plt.legend(handles = [l4, l5, l6, l7], labels = [\"Test_loss\",\"Test_r_loss\",\"Test_kl_loss\",\"BLEU\"], loc = 'best')\n",
    "        plt.show()\n",
    "        \n",
    "    def test_all(self, path, epoch_range, is_bleu=True):\n",
    "        val_loss_list = []\n",
    "        bleu_list = []\n",
    "        for i in range(epoch_range[0], epoch_range[-1]):\n",
    "            self.model.restore(path + str(i))\n",
    "            val_loss = self.test()\n",
    "            val_loss_list.append(val_loss)\n",
    "            if is_bleu:\n",
    "                bleu_score = self.test_bleu()\n",
    "                bleu_list.append(bleu_score)\n",
    "        plt.figure(1)\n",
    "        plt.title('The results') \n",
    "        l1, = plt.plot(val_loss_list,'r')\n",
    "        l2, = plt.plot(bleu_list,'b')\n",
    "        plt.legend(handles = [l1, l2], labels = [\"Test_loss\",\"BLEU\"], loc = 'best')\n",
    "        plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T01:25:14.786133Z",
     "start_time": "2017-12-20T01:25:08.146201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9191\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "top, mid, down = cPickle.load(open('JD/JD_top_mid_down_indices.pkl'))\n",
    "#data = cPickle.load(open('JD/JD_small_indices.pkl'))\n",
    "\n",
    "w2id, id2w = cPickle.load(open('JD/JD_w2id_id2w.pkl'))\n",
    "train_dir = 'char_vae_model/'\n",
    "print len(w2id)\n",
    "#print len(top), len(mid), len(down)\n",
    "\n",
    "data_Y = top + mid + down\n",
    "data_X = [data[:-1] for data in data_Y]\n",
    "data_C = []\n",
    "for t in top:\n",
    "    data_C.append(0)\n",
    "for t in mid:\n",
    "    data_C.append(1)\n",
    "for t in down:\n",
    "    data_C.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T01:56:04.582051Z",
     "start_time": "2017-12-20T01:56:00.533912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data: 526028 | Test_data: 58447 | Batch_size: 256 | Num_batch: 2054 | X_vocab_size: 9191 | Y_vocab_size: 9191\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_EPOCH = 15\n",
    "\n",
    "dp = CVRAE_DP(data_X, data_Y, data_C, w2id, w2id, BATCH_SIZE, n_epoch=NUM_EPOCH)\n",
    "g = tf.Graph() \n",
    "sess = tf.Session(graph=g) \n",
    "with sess.as_default():\n",
    "    with sess.graph.as_default():\n",
    "        model = CVRAE(\n",
    "            dp = dp,\n",
    "            rnn_size = 1024,\n",
    "            latent_dim = 16,\n",
    "            n_layers = 1,\n",
    "            encoder_embedding_dim = 512,\n",
    "            decoder_embedding_dim = 512,\n",
    "            cell_type='gru',\n",
    "            max_infer_length=31,\n",
    "            residual=False,\n",
    "            is_save=True,\n",
    "            beam_width=5,\n",
    "            sess= sess\n",
    "        )\n",
    "        \n",
    "util = CVRAE_util(dp=dp, model=model)\n",
    "#util.fit(train_dir=train_dir, is_bleu=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-19T01:24:06.263Z"
    }
   },
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T01:25:22.045227Z",
     "start_time": "2017-12-20T01:25:22.033974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "货收到了，我很满意。手镯很好。下次还会来的。\n",
      "很不错的一款项链，非常满意。\n",
      "东西很好 就是有点大啊\n",
      "收到宝贝。看着不错也很漂亮。\n",
      "看着价位很不值，戴上还可以。\n",
      "质量太垃圾了\n",
      "相当给力，店主送的宝物俺很喜欢！！！将继续关注啊。\n",
      "项链很漂亮，做工精致光泽度好，喜欢\n",
      "夹子很好夹起来也很好看下次会再买\n",
      "很满意的说\n",
      "这个买回来真心好丑，<UNK>还是错的，下次不买了\n",
      "项链和描述的一样，但是 快递小弟很讨厌\n",
      "？？？？？？？假的，真是一分钱一分货，一点也没错\n",
      "<UNK>们，被骗了，太垃圾了\n",
      "靠 哥们算是被你们 <UNK>爹了\n",
      "难看差评难看差评难看差评\n",
      "掉色严重，两天就花了，不建议购买\n",
      "不满意，态度不行。\n",
      "啥东西。 不好 上当了\n",
      "买的是女款 却给了个男款\n",
      "快递太慢了，一份价钱一份货吧\n",
      "这个一般！\n",
      "好，非常好看，有<UNK><UNK>\n",
      "没用就不见了\n",
      "一般吧，只是一面水晶，后面<UNK>了不太喜欢\n",
      "链子不怎么样，连个首饰盒都没有。\n",
      "质量还可以，还送了小礼物。上面的<UNK>可能<UNK>。\n",
      "……<UNK>\n",
      "物流很难\n",
      "下个<UNK>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "candi = []\n",
    "candi += random.sample(top, 10)\n",
    "candi += random.sample(down, 10)\n",
    "candi += random.sample(mid, 10)\n",
    "\n",
    "candi = [\"\".join([dp.X_id2w[idx] for idx in c[:-1]]) for c in candi]\n",
    "for c in candi:\n",
    "    print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T01:59:43.845728Z",
     "start_time": "2017-12-20T01:59:43.802553Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_transfer(good, bad, candi_list, cls):\n",
    "    z_good = model.xToz(good)\n",
    "    z_bad = model.xToz(bad)\n",
    "    o_good = model.zTox(z_good)[0]\n",
    "    o_bad = model.zTox(z_bad)[0]\n",
    "    z = z_bad - z_good\n",
    "    print 'good:%s | bad:%s' % (o_good, o_bad)\n",
    "    for c in candi_list:\n",
    "        z_c = model.xToz(c)\n",
    "        o_r = model.zTox(z_c, cls)[0]\n",
    "        o_tb = model.zTox(z_c + z, cls)[0]\n",
    "        o_tg = model.zTox(z_c - z, cls)[0]\n",
    "        print \"-----------------------------------------------------------\"\n",
    "        print \"origin: %s | reconstruct: %s\" % (c, o_r)\n",
    "        print \"transfer_g: %s | transfer_b: %s \" % (o_tg, o_tb)\n",
    "        \n",
    "def interpolations(A, B, cls, n=10):\n",
    "    z1 = model.xToz(A)\n",
    "    z2 = model.xToz(B)\n",
    "    z_list = [z1+((z2-z1)/n)*i for i in range(1,n+1)]\n",
    "    print \"-----------------------------------------------------------\"\n",
    "    print A\n",
    "    for z in z_list:\n",
    "        print model.zTox(z, cls)[0]\n",
    "    print B\n",
    "\n",
    "def senti_transfer(A):\n",
    "    z = model.xToz(A)\n",
    "    o1 = model.zTox(z, 0)[0]\n",
    "    o2 = model.zTox(z, 1)[0]\n",
    "    o3 = model.zTox(z, 2)[0]\n",
    "    print \"origin: %s | top: %s | mid: %s | down %s\" % (A, o1, o2, o3)\n",
    "    \n",
    "def generate():\n",
    "    o = model.generate(c=0)\n",
    "    print 'generate top:'\n",
    "    for oo in o:\n",
    "        print '【',oo,'】'\n",
    "    print \"\"\n",
    "    o = model.generate(c=1)\n",
    "    print 'generate mid:'\n",
    "    for oo in o:\n",
    "        print '【',oo,'】'\n",
    "    print \"\"\n",
    "    o = model.generate(c=2)\n",
    "    print 'generate down:'\n",
    "    for oo in o:\n",
    "        print '【',oo,'】'\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T02:01:17.435724Z",
     "start_time": "2017-12-20T01:59:44.342820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /root/VAE/char_vae_model/runs/1513646690/model-9\n",
      "restore /root/VAE/char_vae_model/runs/1513646690/model-9 success\n",
      "generate top:\n",
      "【 质量有保证，喜欢的不得了，很好 】\n",
      "【 包装不错，戒指非常满意。 】\n",
      "【 发货很给力，宝贝收到质量很好，店家人很好很满意的一次购物！ 】\n",
      "\n",
      "generate mid:\n",
      "【 卖家非常非常非常非常非常好，款式很时尚，非常满意的一次网购哦！值得推荐 】\n",
      "【 <UNK>是假的，很大方。欺骗消费者 】\n",
      "【 很精致很小也很小的啊。 】\n",
      "\n",
      "generate down:\n",
      "【 虽然简单，但还是不错的 】\n",
      "【 项链不错哦，女朋友说是真好的说~ 】\n",
      "【 很好，款式新颖，带着很大方，非常漂亮，给个好评 】\n",
      "\n",
      "origin: 货收到了，我很满意。手镯很好。下次还会来的。 | top: 货收到了，快递很给力。戴着很好。满意的一次购物。 | mid: 货收到了，快递很给力。戴着很好的。。。 | down 货收到了，快递很给力。戴着很好的。。。\n",
      "origin: 很不错的一款项链，非常满意。 | top: 很不错的一款宝贝，非常满意。 | mid: 很不错的一款宝贝，非常满意。 | down 很不错的一款宝贝，非常满意。\n",
      "origin: 东西很好 就是有点大啊 | top: 东西很好 就是有点大啊 | mid: 东西很好 就是有点大啊 | down 东西很好 就是有点大啊\n",
      "origin: 收到宝贝。看着不错也很漂亮。 | top: 收到货了。很漂亮哦。也很漂亮 | mid: 收到货了。看起来很漂亮。也不错 | down 收到项链还可以。不满意。\n",
      "origin: 看着价位很不值，戴上还可以。 | top: 总体来说还好，带上也很好看？ | mid: 还阔以，感觉也不是很好看 | down 项链有点细，总体还算满意。\n",
      "origin: 质量太垃圾了 | top: 质量太垃圾了， | mid: 质量太垃圾了， | down 质量太垃圾了，\n",
      "origin: 相当给力，店主送的宝物俺很喜欢！！！将继续关注啊。 | top: 店主人很棒，给同事买的生日礼物！她很喜欢！客服也有耐心！ | mid: 服务超好，送女友的最佳礼物，她很喜欢！全5分好评！ | down 服务极差，忽悠人的礼物！她很喜欢！以后再也不回京东！\n",
      "origin: 项链很漂亮，做工精致光泽度好，喜欢 | top: 项链很漂亮，做工精致光泽度好，喜欢 | mid: 项链很漂亮，做工精致光泽度好，喜欢 | down 项链很漂亮，做工精致光泽度好，喜欢\n",
      "origin: 夹子很好夹起来也很好看下次会再买 | top: 看起来很好女朋友很喜欢这款<UNK><UNK> | mid: 耳钉很好很好看但是自己觉得很便宜。 | down 觉得很好女朋友很喜欢上面<UNK>空<UNK>\n",
      "origin: 很满意的说 | top: 很满意满意的说 | mid: 很满意满意的说 | down 很满意满意的说\n",
      "origin: 这个买回来真心好丑，<UNK>还是错的，下次不买了 | top: 这个价位很划算，本来是我买的那个小饰品，很满意 | mid: 这个项链很普通，这个价钱能买到这样的东西，没有图片好看 | down 这个价位很不值这个价，我还不如路边摊买的，没什么失望\n",
      "origin: 项链和描述的一样，但是 快递小弟很讨厌 | top: 项链很漂亮！和想想中的差不多，<UNK><UNK>的！ | mid: 项链和描述的一样，但是快递不给力！很讨厌 | down 项链跟描述的一样！就是快递很慢！不知道是不是真的\n",
      "origin: ？？？？？？？假的，真是一分钱一分货，一点也没错 | top: 垃圾？？？？？？？？？？？？？？？？？？？？？，，，， | mid: 垃圾？？？？？？？？？？？？？？？？？？？，，，，， | down 垃圾？？？？？？？？？？？？？？？？？？？，，，，，\n",
      "origin: <UNK>们，被骗了，太垃圾了 | top: <UNK><UNK>死了，真垃圾，太粗糙了 | mid: <UNK><UNK>太明显了，像塑料的，，， | down <UNK><UNK>死了，真垃圾，真垃圾\n",
      "origin: 靠 哥们算是被你们 <UNK>爹了 | top: 骗子 法国<UNK>10个字形容了 | mid: 差评 为我这种东西 <UNK>在一起吧 | down 差评 把sb<UNK>10个字形容了\n",
      "origin: 难看差评难看差评难看差评 | top: 完美，带上简直帅帅的一模一样 | mid: 难看差评差评难看死的 | down 难看差评差评差评难看死\n",
      "origin: 掉色严重，两天就花了，不建议购买 | top: 掉色了，不小心掉地上，又实惠 | mid: 掉色厉害，刚带没多久就变色了，太差劲了 | down 掉色，刚带花都掉了，太厉害了\n",
      "origin: 不满意，态度不行。 | top: 不满意，商品还可以。 | mid: 不满意，商品还可以。 | down 不满意，商品不行。\n",
      "origin: 啥东西。 不好 上当了 | top: 质量不好，1个月的 | mid: 质量不好，1个月的 | down 质量不好？？？破玩意，\n",
      "origin: 买的是女款 却给了个男款 | top: 帮别人买的 这是我知道什么样的 | mid: 买的是尾戒 结果只有这个价位 | down 买的是情侣 结果只有这个样\n",
      "origin: 快递太慢了，一份价钱一份货吧 | top: 快递太慢了，一分价钱一分货吧 | mid: 快递太慢了，一分价钱一分货吧 | down 快递太慢了，一分价钱一分货吧\n",
      "origin: 这个一般！ | top: 就是好！ | mid: 觉得一般！ | down 觉得一般！\n",
      "origin: 好，非常好看，有<UNK><UNK> | top: 好看，特别好。客服态度也很好，以后会常买 | mid: 好，特别好看。客服态度很好，尤其是<UNK>了 | down 不好，特别好看。质量很好。尤其是快递特别慢\n",
      "origin: 没用就不见了 | top: 没用就不见了 | mid: 没用就不见了 | down 没用就不见了\n",
      "origin: 一般吧，只是一面水晶，后面<UNK>了不太喜欢 | top: 一般吧，虽然不是图片上那么好看，价钱也懒得退了。 | mid: 一般吧，虽然没有图片上那么好看，这个价钱也不会用 | down 一般吧，虽然不是图片上那么好看，而且还将就断了。\n",
      "origin: 链子不怎么样，连个首饰盒都没有。 | top: 看着像假，而且东西不咋滴。 | mid: 手工粗糙，一看就不咋地的。 | down 一看就那样，感觉和照片差远了。。。\n",
      "origin: 质量还可以，还送了小礼物。上面的<UNK>可能<UNK>。 | top: 看着还可以。还送了小礼品。唯一不足的东西也很好。唯独被压坏了。 | mid: 看着还可以。就是小了点。<UNK>的人都说很好。总的来说还可以吧 | down 看着还可以。就是小了点。<UNK>的人都说还可以。送货员态度很好\n",
      "origin: ……<UNK> | top: ……<UNK><UNK><UNK> | mid: ……<UNK><UNK><UNK> | down ……<UNK><UNK><UNK>\n",
      "origin: 物流很难 | top: 物流超慢 | mid: 物流超慢 | down 物流超慢\n",
      "origin: 下个<UNK> | top: 仅仅<UNK><UNK><UNK> | mid: 仅仅<UNK><UNK><UNK> | down 仅仅<UNK><UNK><UNK>\n",
      "INFO:tensorflow:Restoring parameters from /root/VAE/char_vae_model/runs/1513646690/model-10\n",
      "restore /root/VAE/char_vae_model/runs/1513646690/model-10 success\n",
      "generate top:\n",
      "【 商品质量好！物流很快！昨天下单今天就到了！还送了好多水钻！谢谢店家！ 】\n",
      "【 质量还是很好的。 】\n",
      "【 用不了，就是太小了 】\n",
      "\n",
      "generate mid:\n",
      "【 发货太慢了，带着也不好看！ 】\n",
      "【 送人的 希望大家都能喜欢。 】\n",
      "【 挺好的。。。。。 】\n",
      "\n",
      "generate down:\n",
      "【 发货速度非常满意，东西是正品，物超所值， 】\n",
      "【 不是很好的东西，很好看 】\n",
      "【 第一次买 很好 就是有点小了 】\n",
      "\n",
      "origin: 货收到了，我很满意。手镯很好。下次还会来的。 | top: 货收到了。很喜欢。。。。。。。。。。。。。。。。。。。。。。。。。 | mid: 货收到了。很喜欢。。。。。。。。。。。。。。。。。。 | down 货收到了，我很喜欢。。。很满意的一次购物。\n",
      "origin: 很不错的一款项链，非常满意。 | top: 很不错的一款项链，非常满意。 | mid: 很不错的一款项链，非常满意。 | down 很不错的一款项链，非常满意。\n",
      "origin: 东西很好 就是有点大啊 | top: 东西很好 就是有点大 | mid: 东西很好 就是有点大 | down 东西很好 就是有点大\n",
      "origin: 收到宝贝。看着不错也很漂亮。 | top: 收到货了。外观很漂亮。色泽很不错。 | mid: 手链很漂亮。项链看着还不错。 | down 手链很漂亮。项链看着还不错。\n",
      "origin: 看着价位很不值，戴上还可以。 | top: 拿到手还不错，戴上有点<UNK> | mid: 吊坠还行，戴上还可以。 | down 手链带起来还行，<UNK>有毒。\n",
      "origin: 质量太垃圾了 | top: 质量太垃圾了 | mid: 质量太垃圾了 | down 质量太垃圾了\n",
      "origin: 相当给力，店主送的宝物俺很喜欢！！！将继续关注啊。 | top: 给老婆买的，她很喜欢，以后会多继续光顾！\\n | mid: 给老婆买的，她很喜欢！无语！请尽快关注 | down 给老婆买的，她很喜欢！京东自营与实物相符！\n",
      "origin: 项链很漂亮，做工精致光泽度好，喜欢 | top: 项链很漂亮，做工精致，戴着很合适 | mid: 项链很漂亮，做工精致，戴着很好 | down 项链很漂亮，做工精致，戴着很好\n",
      "origin: 夹子很好夹起来也很好看下次会再买 | top: 发卡很好很好看但是我很有眼光 | mid: 不是很好连子很好老婆很喜欢。 | down 不是很好连子也很好&hellip;\n",
      "origin: 很满意的说 | top: 很满意的说了 | mid: 很满意的说了 | down 很满意的说了\n",
      "origin: 这个买回来真心好丑，<UNK>还是错的，下次不买了 | top: 这个价位也很划算，买了一堆，送的礼品也很棒 | mid: 这个价位也很划算，买的时候没有注意，就别换了 | down 这个价位也很划算，买的时候还不如不送，太失望了\n",
      "origin: 项链和描述的一样，但是 快递小弟很讨厌 | top: 宝贝很漂亮，比我预想的要大些，总体来说很满意！ | mid: 项链和描述的一样，但是物流好慢！很给力！ | down 项链和描述的一样，但是物流好慢！很给力！\n",
      "origin: ？？？？？？？假的，真是一分钱一分货，一点也没错 | top: ？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？ | mid: ？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？ | down ？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？\n",
      "origin: <UNK>们，被骗了，太垃圾了 | top: <UNK>太厉害了，大家不要买，物流也快， | mid: <UNK>太多了，<UNK>擦亮眼睛，直接扔了， | down <UNK>太垃圾了，大家不要上当，<UNK>了，\n",
      "origin: 靠 哥们算是被你们 <UNK>爹了 | top: 好评 如果能<UNK>的话就更完美了 | mid: 哎 我以为有一条黑线 这个价格还行 | down 差评 给我刻个<UNK> 其馀都嫌多\n",
      "origin: 难看差评难看差评难看差评 | top: 太漂亮了，高端大气上档次<UNK>的 | mid: 太次了。 反差特别丑的 | down 差评差评差评差评差评……超难看\n",
      "origin: 掉色严重，两天就花了，不建议购买 | top: 上当了，戴上不舒服，完全超出期望值。 | mid: 掉色了，一点都不对，而且又丑。 | down 掉色了，一点都不对，又丑。\n",
      "origin: 不满意，态度不行。 | top: 不满意，速度太慢。 | mid: 不满意，速度太慢。 | down 不满意，没收到货。\n",
      "origin: 啥东西。 不好 上当了 | top: 什么东西，有一个坏了 | mid: 什么东西，一个星期坏了 | down 什么东西，一个星期坏了\n",
      "origin: 买的是女款 却给了个男款 | top: 买的时候还行吧 实物和图片不一样 | mid: 买的项链是一样的 不过快递太差了 | down 买的手链是不锈钢的 没看见这个价钱\n",
      "origin: 快递太慢了，一份价钱一份货吧 | top: 快递太慢了，一分价钱一份货吧 | mid: 快递太慢了，一分价钱一份货吧 | down 物流太慢了，15号可以理解\n",
      "origin: 这个一般！ | top: 赠品好好好 | mid: 赠品一般 | down 赠品一般\n",
      "origin: 好，非常好看，有<UNK><UNK> | top: 好。特别好看，质量也很好，有需要还会来的。 | mid: 好，特别好看。而且质量不是很好的那种产品。 | down 不好，特别不好。质量也很好。最重要的那种\n",
      "origin: 没用就不见了 | top: <UNK>就生锈了 | mid: <UNK>就生锈了 | down <UNK>就生锈了\n",
      "origin: 一般吧，只是一面水晶，后面<UNK>了不太喜欢 | top: 比想象中小，不过还好，如果<UNK>的话就更完美了。 | mid: 一般吧，实物比图片还要好看，带起来有点不舒服。 | down 小了点，不知道是不是银，也不是施华洛世奇。\n",
      "origin: 链子不怎么样，连个首饰盒都没有。 | top: 物流好慢。跟照片上不一样。 | mid: 物流好慢，项链和照片上不一样。 | down 物流好慢。跟照片上不一样。\n",
      "origin: 质量还可以，还送了小礼物。上面的<UNK>可能<UNK>。 | top: 质量还可以。送的小礼物也很好。就是绳子太短了。。？？？？？？ | mid: 还算可以。就是小了点。。。<UNK>的<UNK><UNK><UNK><UNK><UNK>。 | down 还算可以。就是小了点。。。<UNK>的<UNK><UNK><UNK><UNK><UNK>。\n",
      "origin: ……<UNK> | top: ……<UNK><UNK><UNK> | mid: ……<UNK><UNK><UNK> | down ……<UNK><UNK><UNK>\n",
      "origin: 物流很难 | top: 物流有点慢 | mid: 物流太差劲了 | down 物流太差劲了\n",
      "origin: 下个<UNK> | top: <UNK><UNK><UNK><UNK> | mid: <UNK><UNK><UNK><UNK> | down <UNK><UNK><UNK><UNK>\n",
      "INFO:tensorflow:Restoring parameters from /root/VAE/char_vae_model/runs/1513646690/model-11\n",
      "restore /root/VAE/char_vae_model/runs/1513646690/model-11 success\n",
      "generate top:\n",
      "【 很喜欢，昨天下单今天就到了，发货速度快 】\n",
      "【 送货速度很快，东西也不错，挺好的！ 】\n",
      "【 戒指很精致也很漂亮 做工也很精致 】\n",
      "\n",
      "generate mid:\n",
      "【 物流挺快的，链子与图片不一样，而且不是银的。 】\n",
      "【 项链真的是很好的吊坠，做工也很精致，女朋友很喜欢 】\n",
      "【 发货快，服务好，很满意，以后会继续光顾的！ 】\n",
      "\n",
      "generate down:\n",
      "【 项链质量很差，包装也很难看！ 】\n",
      "【 很丑，假的，很喜欢 】\n",
      "【 很普通~~~~~~~~ 】\n",
      "\n",
      "origin: 货收到了，我很满意。手镯很好。下次还会来的。 | top: 宝贝收到了。卖家服务态度很好，东西也很满意的一次购物。 | mid: 宝贝收到了。快递很给力。质量很好。和图片上的一样。 | down 货收到了，我很满意的一次购物。快递员态度很恶劣。\n",
      "origin: 很不错的一款项链，非常满意。 | top: 很不错的宝贝，朋友很喜欢 | mid: 很不错的一次购物，朋友很喜欢 | down 很不错的项链，朋友都说很好看\n",
      "origin: 东西很好 就是有点大啊 | top: 东西挺好的 就是有点大 | mid: 东西挺好的 就是有点小 | down 东西挺好的 就是有点小\n",
      "origin: 收到宝贝。看着不错也很漂亮。 | top: 外观很漂亮。质量也很好。 | mid: 外观还不错。。。。。。。。。。。。 | down 外观还不错。。。。。。。。。。。。。\n",
      "origin: 看着价位很不值，戴上还可以。 | top: 项链很好看，对得起这个价。 | mid: 链子很细，打结。。。。 | down 链子很细，打结。。。。。\n",
      "origin: 质量太垃圾了 | top: 质量太垃圾了 | mid: 质量太垃圾了 | down 质量太垃圾了\n",
      "origin: 相当给力，店主送的宝物俺很喜欢！！！将继续关注啊。 | top: 很喜欢，发货速度快，值得购买！谢谢店家送的小礼物！ | mid: 很喜欢，发货速度快，下次还会再来！！！！！！！！！！！！！！！！！！！！ | down 真心不想给！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
      "origin: 项链很漂亮，做工精致光泽度好，喜欢 | top: 项链很漂亮，水晶很闪，喜欢 | mid: 项链很漂亮，水晶很闪，喜欢 | down 项链很漂亮，水晶很闪，喜欢\n",
      "origin: 夹子很好夹起来也很好看下次会再买 | top: 质量很好很喜欢```````` | mid: 项链很好很喜欢\n",
      "就是物流太慢了 | down 质量不是很好\n",
      "老婆很喜欢\n",
      "origin: 很满意的说 | top: 送朋友的 | mid: <UNK>过的 | down <UNK>过的\n",
      "origin: 这个买回来真心好丑，<UNK>还是错的，下次不买了 | top: 这个价格很划算，比我想象中的要好很多 | mid: 这个价位真的很一般，而且快递员的态度不怎么样 | down 这个价位真的很差，而且<UNK>的东西也不好，建议不要购买\n",
      "origin: 项链和描述的一样，但是 快递小弟很讨厌 | top: 宝贝挺好的，包装也很高大上，给个好评！ | mid: 项链很好，是我想要的那种~~~~~ | down 项链很好，和图片描述的一样 快递也给力\n",
      "origin: ？？？？？？？假的，真是一分钱一分货，一点也没错 | top: ？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？ | mid: ？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？ | down ？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: <UNK>们，被骗了，太垃圾了 | top: 超级好看，这款项链很好看，快递也很快， | mid: 超级无敌<UNK>，真的是醉了，很丑！ | down 超级无敌丑，真的是<UNK>，<UNK>了！\n",
      "origin: 靠 哥们算是被你们 <UNK>爹了 | top: 好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好 | mid: <UNK> 让我等了好久才来评价 | down 差评 我等了好久才来评价 <UNK>\n",
      "origin: 难看差评难看差评难看差评 | top: 满意，同事们都说好看 | mid: 假货。同事们都说好看 | down 假货。跟图片上一样\n",
      "origin: 掉色严重，两天就花了，不建议购买 | top: 太差劲了，链子细得可怜，不建议购买 | mid: 太容易刮花了，而且还不对称，不值得购买 | down 太差劲了，一扯就褪色了，劝别买\n",
      "origin: 不满意，态度不行。 | top: 不错，两天就到了。 | mid: 不满意，送货太慢了。 | down 不满意，送货太慢了。\n",
      "origin: 啥东西。 不好 上当了 | top: 质量不好，物流慢，5天才到！ | mid: 质量不好，送货速度太慢！ | down 质量不好，差评 差评差评差评\n",
      "origin: 买的是女款 却给了个男款 | top: 我要的是情侣号 结果给我发错了 | mid: 说的25号 结果是18厘米的 | down 说的25号 结果是18厘米的\n",
      "origin: 快递太慢了，一份价钱一份货吧 | top: 送货速度太慢了，10天才到。 | mid: 送货速度太慢了，10号才到。 | down 快递太慢了，4天才到2天\n",
      "origin: 这个一般！ | top: 这个还好吧！ | mid: 东西一般吧！ | down 东西一般吧！\n",
      "origin: 好，非常好看，有<UNK><UNK> | top: 好看，很有个性，要是<UNK>的话就更好了 | mid: 好看，很有个性，要是<UNK>的话就更好了 | down 不好，很容易断，而且又大又短又<UNK>\n",
      "origin: 没用就不见了 | top: 哈哈哈哈，很强势 | mid: 将就用，不耐用 | down 一扎就断了，\n",
      "origin: 一般吧，只是一面水晶，后面<UNK>了不太喜欢 | top: 一般吧，和照片上不一样，不过这个价钱还行 | mid: 一般吧，没照片上那么好看，这个价钱也就这样了 | down 一般吧，和地摊上*****，有点不一样\n",
      "origin: 链子不怎么样，连个首饰盒都没有。 | top: 总体来说还可以，就是不知道是不是银的 | mid: 总体来说还可以，就是不知道是不是纯银的 | down 链子太细了，扫码是塑料做的\n",
      "origin: 质量还可以，还送了小礼物。上面的<UNK>可能<UNK>。 | top: 还不错。就是有点小了。不过总的来说还是挺好的。快递员态度很好。 | mid: 还算可以。就是有点小。送的东西也无所谓了。 | down 还算可以。就是有点小。送的东西也不怎么好。\n",
      "origin: ……<UNK> | top: ？<UNK>的<UNK><UNK> | mid: 假的<UNK><UNK><UNK> | down 假的<UNK><UNK><UNK>\n",
      "origin: 物流很难 | top: 大小正合适 | mid: 大小不合 | down 严重不怎么样\n",
      "origin: 下个<UNK> | top: 热热热热热热<UNK><UNK><UNK> | mid: 热热热热热热<UNK><UNK><UNK> | down 热热热热热热<UNK><UNK><UNK>\n",
      "INFO:tensorflow:Restoring parameters from /root/VAE/char_vae_model/runs/1513646690/model-12\n",
      "restore /root/VAE/char_vae_model/runs/1513646690/model-12 success\n",
      "generate top:\n",
      "【 挺好的。。。。。。。。。。。。。。。。。。。。。 】\n",
      "【 质量不错，价格实惠，带着很舒服！ 】\n",
      "【 东西不错，质量还是可以的，毕竟是一分钱一分货 】\n",
      "\n",
      "generate mid:\n",
      "【 东西跟图片不一样 】\n",
      "【 好漂亮的项链，是纯银的，很满意的一次购物，赞一个 】\n",
      "【 卖家服务好，发货速度快，值得购买！ 】\n",
      "\n",
      "generate down:\n",
      "【 很好，质量很好，价格很实惠，很满意的一次购物！ 】\n",
      "【 物流太慢。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 】\n",
      "【 东西还行，物流慢了点 】\n",
      "\n",
      "origin: 货收到了，我很满意。手镯很好。下次还会来的。 | top: 宝贝收到了，质量很好，物流也很给力。 | mid: 货收到了，质量很好，物流也很给力。 | down 货收到了，质量很好，物流也很给力。\n",
      "origin: 很不错的一款项链，非常满意。 | top: 很满意的一次购物，值得推荐。 | mid: 很满意的一次购物。 | down 很不错的一次购物经历。\n",
      "origin: 东西很好 就是有点大啊 | top: 东西挺好的就是不知道会不会退色 | mid: 感觉挺好的就是不知道会不会退色 | down 感觉不怎么好～～～～～\n",
      "origin: 收到宝贝。看着不错也很漂亮。 | top: 手链很漂亮。卖家还送了小礼物。 | mid: 手链很漂亮。还送了耳钉。 | down 手链还可以。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 看着价位很不值，戴上还可以。 | top: 物流很快，宝贝也很不错哦 | mid: 物流很快，项链也很好看 | down 物流慢不说，东西也不是很满意\n",
      "origin: 质量太垃圾了 | top: 感觉被坑了 | mid: 感觉被坑了 | down 感觉被坑了\n",
      "origin: 相当给力，店主送的宝物俺很喜欢！！！将继续关注啊。 | top: 好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好好 | mid: 给别人买的，他说还可以吧，没有想象中那么好！ | down ******的东西，让我失望，还不如不送呢！\n",
      "origin: 项链很漂亮，做工精致光泽度好，喜欢 | top: 质量很好，做工精细，外观漂亮 | mid: 项链不错，就是吊坠太重了 | down 质量好差，跟图片上不一样\n",
      "origin: 夹子很好夹起来也很好看下次会再买 | top: 摸起来<UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK> | mid: 个人认为<UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK> | down 个人觉得<UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>\n",
      "origin: 很满意的说 | top: 很满意 | mid: 很满意 | down 很满意\n",
      "origin: 这个买回来真心好丑，<UNK>还是错的，下次不买了 | top: 这个价格能买到这么好的东西，很漂亮，很精致 | mid: 这个价格能买到这么好的东西，有点失望 | down 这个价格能不能奢望这么好的东西，不知道是不是纯银的\n",
      "origin: 项链和描述的一样，但是 快递小弟很讨厌 | top: 实物和图片一样，是我想要的那种！ | mid: 实物和图片上的不一样，快递也很慢 | down 实物和图片上的不一样，快递也很慢\n",
      "origin: ？？？？？？？假的，真是一分钱一分货，一点也没错 | top: 美美哒，做工精致，戴上很好看，发货速度快，好评 | mid: 假的，做工糙，不知道是不是真的，<UNK> | down 假的，而且不好看，货不对板\n",
      "origin: <UNK>们，被骗了，太垃圾了 | top: 太差劲了，简直是我想要的那种，<UNK>了。 | mid: 和图片上的不太一样，<UNK>了，不直这个价 | down 太差劲了，我明明是<UNK>的，真想退货了。\n",
      "origin: 靠 哥们算是被你们 <UNK>爹了 | top: 还好吧 毕竟被我弄丢了？？？？？？？？？？？？？？？？？？？？？？ | mid: 还好吧 刚拿回来就被别人拿走了 | down 垃圾东西 怎么说呢 客服态度也不好\n",
      "origin: 难看差评难看差评难看差评 | top: 高端大气上档次，低调奢华的<UNK> | mid: 与图片上的不太一样 | down 差评 和图片上的不一样\n",
      "origin: 掉色严重，两天就花了，不建议购买 | top: 已经收到了，戴上很有气质，做工精细，好评 | mid: 带了一周就掉色了，无语，无语 | down 带了一周就掉色了，无语，差评！\n",
      "origin: 不满意，态度不行。 | top: 不错，物流很快，客服态度很好 | mid: 不错，就是物流太慢，客服态度不好 | down 不好，一点都不好，快递员态度不好\n",
      "origin: 啥东西。 不好 上当了 | top: 东西不错，就是物流太慢了 | mid: 东西不错，就是物流太慢了 | down 质量不好，差评\n",
      "origin: 买的是女款 却给了个男款 | top: 送朋友的 朋友看了一下 感觉挺不错的 | mid: 送朋友的 朋友看了一下 感觉不值这个价 | down 买了两条 送的转运珠都变成棕色了\n",
      "origin: 快递太慢了，一份价钱一份货吧 | top: 快递太慢了……………………………………&hellip; | mid: 快递太慢了………………………………&hellip; | down 快递太慢了………………………………&hellip;\n",
      "origin: 这个一般！ | top: 东西不错 值这个价 | mid: 感觉还行吧！ | down 感觉被坑了\n",
      "origin: 好，非常好看，有<UNK><UNK> | top: 好。很好。特别好看。质量也很好。 | mid: 好，很好看。就是<UNK>了点。 | down 不好。特别不好。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 没用就不见了 | top: 就那回事了 | mid: 太慢了 | down 太慢了\n",
      "origin: 一般吧，只是一面水晶，后面<UNK>了不太喜欢 | top: 一般吧，没有想象中那么好，不过这个价钱也很划算。 | mid: 一般吧，没有想象中那么好，不过这个价钱也就这样了。 | down 一般吧，没有想象中那么好，不过这个价钱也就这样了。\n",
      "origin: 链子不怎么样，连个首饰盒都没有。 | top: 还不错，就是发货速度有点慢。 | mid: 还可以，就是不知道是不是银的。 | down 一点也不满意。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 质量还可以，还送了小礼物。上面的<UNK>可能<UNK>。 | top: 质量挺好的，做工也很好。还送了小礼物。 | mid: 质量一般，没有想象中的那么好。而且还送了小礼物。 | down 质量一般。没有想象中的那么好。而且还送了小礼物。\n",
      "origin: ……<UNK> | top: <UNK>@@@@@@@@@ | mid: @@@@@@@@@ | down <UNK>@@@@@@@\n",
      "origin: 物流很难 | top: 物流给力 | mid: 物流超级慢 | down 物流超级慢\n",
      "origin: 下个<UNK> | top: <UNK><UNK> | mid: <UNK><UNK> | down <UNK><UNK>\n",
      "INFO:tensorflow:Restoring parameters from /root/VAE/char_vae_model/runs/1513646690/model-13\n",
      "restore /root/VAE/char_vae_model/runs/1513646690/model-13 success\n",
      "generate top:\n",
      "【 满意的一次购物。 】\n",
      "【 没毛病 】\n",
      "【 宝贝收到了，质量很好，和图片上的一样，很喜欢，好评！ 】\n",
      "\n",
      "generate mid:\n",
      "【 不多说，很喜欢 】\n",
      "【 送给朋友的生日礼物，她很喜欢 】\n",
      "【 一般吧！！！！！！！！！！！！！！！！！！！！！！！！！！！！！ 】\n",
      "\n",
      "generate down:\n",
      "【 物流太慢了 】\n",
      "【 发货速度很快，包装也很精美，是纯银的，不知道会不会褪色。 】\n",
      "【 物流慢了点给个差评吧 】\n",
      "\n",
      "origin: 货收到了，我很满意。手镯很好。下次还会来的。 | top: 宝贝收到了，质量很好，和卖家描述的一样，很满意。 | mid: 东西收到了，和图片上的一样，很喜欢。 | down 东西收到了。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 很不错的一款项链，非常满意。 | top: 质量很好，和卖家描述的一样，很满意 | mid: 质量很好，和描述的一样，很满意 | down 一点都不好，和图片上的不一样\n",
      "origin: 东西很好 就是有点大啊 | top: 很好 老婆很喜欢 快递也很给力 | mid: 很好 老婆很喜欢�鳘鳘鳘鳘鳘鳘鳘鳘鳘鳘� | down 不是很好 但是老婆很喜欢\n",
      "origin: 收到宝贝。看着不错也很漂亮。 | top: 手链很漂亮，水晶很闪。 | mid: 还可以，看着还行。 | down 还没戴，不知道效果怎么样\n",
      "origin: 看着价位很不值，戴上还可以。 | top: 总体来说还可以，就是物流有点慢 | mid: 总体来说还可以，就是物流有点慢 | down 物流太慢了，东西也不咋滴\n",
      "origin: 质量太垃圾了 | top: 质量很好价格实惠值得购买 | mid: 质量很好价格实惠 | down 质量太差\n",
      "origin: 相当给力，店主送的宝物俺很喜欢！！！将继续关注啊。 | top: 很好！！！！！！！！！！！！！！！！！！！！！！！！！！！！！ | mid: 还可以吧！！！！！！！！！！！！！！！！！！！！！！！！！！！！ | down 差评！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
      "origin: 项链很漂亮，做工精致光泽度好，喜欢 | top: 做工精致，款式漂亮，戴上很好看，很喜欢！ | mid: 做工精致，款式漂亮，很喜欢！ | down 差评！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
      "origin: 夹子很好夹起来也很好看下次会再买 | top: 很好很好很喜欢 | mid: 不是很好很喜欢 | down 不是很好&hellip;\n",
      "origin: 很满意的说 | top: 东西不错 | mid: 东西还不错 | down 东西还行\n",
      "origin: 这个买回来真心好丑，<UNK>还是错的，下次不买了 | top: 就这样吧，毕竟价格在那摆着玩的 | mid: 感觉像***************************** | down 感觉不值这个价，**************的东西\n",
      "origin: 项链和描述的一样，但是 快递小弟很讨厌 | top: 项链很漂亮，做工也很精致，价格也不贵，非常满意！ | mid: 实物和图片相差较大，而且不咋地！ | down 实物和图片相差甚远，而且也不咋地！\n",
      "origin: ？？？？？？？假的，真是一分钱一分货，一点也没错 | top: ，，，，，，，？？？？？？？？？？？？？？？？？？？？？？？？ | mid: ？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？ | down ？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？\n",
      "origin: <UNK>们，被骗了，太垃圾了 | top: <UNK> <UNK> <UNK> <UNK> <UNK>死了 | mid: ***<UNK>，<UNK><UNK>就掉了！ | down ***<UNK>，<UNK><UNK>就掉了！\n",
      "origin: 靠 哥们算是被你们 <UNK>爹了 | top: ------------------------------- | mid: 有瑕疵 送货速度也太慢了 | down 无语 一点也不好 劝大家不要买\n",
      "origin: 难看差评难看差评难看差评 | top: 高端大气上档次 | mid: 与图片相差太大 | down 差评差评差评差评差评差评差评差评\n",
      "origin: 掉色严重，两天就花了，不建议购买 | top: 太差劲了，带两天就变黑了 | mid: 太垃圾了，带两天就变黑了 | down 太垃圾了，带两天就变黑了\n",
      "origin: 不满意，态度不行。 | top: 不错，很漂亮。 | mid: 不满意，太小了 | down 不满意，太差了\n",
      "origin: 啥东西。 不好 上当了 | top: 很好 质量很好 发货速度很快 | mid: 还行 就是不知道会不会掉色 | down 不好 一点都不好\n",
      "origin: 买的是女款 却给了个男款 | top: 给女朋友买的 她挺喜欢的 | mid: 给朋友买的 还行吧 | down 送的东西太假了\n",
      "origin: 快递太慢了，一份价钱一份货吧 | top: 快递很快，2号就到了 | mid: 快递太慢了，不过一分价钱一分货 | down 我只能说一分价钱一份货，一份价钱一份货，唉\n",
      "origin: 这个一般！ | top: 非常好！ | mid: 还可以！！！！！ | down 质量太差了！\n",
      "origin: 好，非常好看，有<UNK><UNK> | top: 挺好看的。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | mid: 不好。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | down 不好。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 没用就不见了 | top: 不知道怎么用 | mid: 不知道怎么用 | down 不好不好\n",
      "origin: 一般吧，只是一面水晶，后面<UNK>了不太喜欢 | top: 挺漂亮的，就是稍微有点小，再大一点就更好了。 | mid: 一般般吧，没有想象中那么好，感觉不值这个价。 | down 太小了，根本不是那么多钱，而且还那么贵。\n",
      "origin: 链子不怎么样，连个首饰盒都没有。 | top: 东西挺好的，发货速度也快。 | mid: 东西挺好的，发货速度也快。 | down 实物和图片相差太远，感觉不值这个价\n",
      "origin: 质量还可以，还送了小礼物。上面的<UNK>可能<UNK>。 | top: 不错。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | mid: 还行。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | down 还行。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: ……<UNK> | top: <UNK> | mid: <UNK><UNK> | down <UNK><UNK>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 物流很难 | top: 很好看，很喜欢 | mid: <UNK> | down <UNK>\n",
      "origin: 下个<UNK> | top: 和图上的不一样 | mid: 和图片上的不一样 | down 和图片上完全不一样\n",
      "INFO:tensorflow:Restoring parameters from /root/VAE/char_vae_model/runs/1513646690/model-14\n",
      "restore /root/VAE/char_vae_model/runs/1513646690/model-14 success\n",
      "generate top:\n",
      "【 挺好的，就是大了点 】\n",
      "【 质量挺好的，颜色也很正 】\n",
      "【 还可以，就是有点小贵 】\n",
      "\n",
      "generate mid:\n",
      "【 刚买的时候还没带过几天就掉色了 】\n",
      "【 感觉不值这个价！！！！！！！！！！！！！！！！！！！！！！！！！！！ 】\n",
      "【 假的是假的。。。。。。。。。。。。。。。。。。。。。。。。。。 】\n",
      "\n",
      "generate down:\n",
      "【 ******************************* 】\n",
      "【 没有想像中的那么好 】\n",
      "【 差评 差评 差评 差评 差评 差评差评差评差评差评差评差评差评差评 】\n",
      "\n",
      "origin: 货收到了，我很满意。手镯很好。下次还会来的。 | top: 质量不错，卖家服务态度好，物流也快，下次还会再来的 | mid: 还可以，就是快递太慢了，等了四天才到 | down 还可以，就是快递太慢了，等的花儿都谢了\n",
      "origin: 很不错的一款项链，非常满意。 | top: 质量很好，做工精致，款式漂亮，价格实惠，值得购买。 | mid: 不咋滴。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | down 不咋地。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 东西很好 就是有点大啊 | top: 东西挺好的 女朋友很喜欢 就是物流有点慢 | mid: 东西挺好的 就是链子有点细 | down 东西挺好的 就是不知道会不会掉色\n",
      "origin: 收到宝贝。看着不错也很漂亮。 | top: 还可以，就是不知道会不会掉色 | mid: 还可以，就是不知道会不会掉色 | down 还可以就是不知道时间长了会不会掉色\n",
      "origin: 看着价位很不值，戴上还可以。 | top: 手链很漂亮，做工精致，戴在手上很好看，很喜欢。 | mid: 链子太细了。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | down 差评。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 质量太垃圾了 | top: 质量很好 做工精细 款式漂亮 | mid: 质量太差 | down 质量太差\n",
      "origin: 相当给力，店主送的宝物俺很喜欢！！！将继续关注啊。 | top: 买给老婆的生日礼物，老婆很喜欢，以后有需要还会再来的，祝老板生意兴隆 | mid: 帮朋友买的，她说还可以，就是送货速度太慢了！！！！！！！！！！！！！！！！ | down 只能说一句话，这东西还不如不送呢！！！！！！！！！！！！！！！！！！！！\n",
      "origin: 项链很漂亮，做工精致光泽度好，喜欢 | top: 项链很漂亮，做工精致，色泽也不错，很喜欢 | mid: 外观漂亮，做工精致，色泽很不错。 | down 比想象中小很多，但还可以\n",
      "origin: 夹子很好夹起来也很好看下次会再买 | top: 东西很好，我很喜欢。下次再来。 | mid: 我很喜欢。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | down 我很喜欢。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 很满意的说 | top: 很满意的一次购物 | mid: 不是很满意 | down 不是很满意\n",
      "origin: 这个买回来真心好丑，<UNK>还是错的，下次不买了 | top: 这个价格买到这样的东西，挺好的 | mid: 这个价钱买到这样的东西，就不评了 | down 这是我买的最差的东西，便宜没好货\n",
      "origin: 项链和描述的一样，但是 快递小弟很讨厌 | top: 宝贝收到了，质量很好，和卖家描述的一样，很喜欢 | mid: 这款项链还可以，就是不知道是不是纯银的 | down 我都没有收到这个项链，怎么说呢\n",
      "origin: ？？？？？？？假的，真是一分钱一分货，一点也没错 | top: 挺漂亮的，做工精致，价格也不贵，值得购买 | mid: 不值这个价，，，，，，，，，，，，，，，，，，，，，，，，，，，， | down ***********，，，，，，，，，，，，，，，，，，，，\n",
      "origin: <UNK>们，被骗了，太垃圾了 | top: 宝贝收到了，是正品，包装精美，款式时尚，戴上很漂亮，满意。 | mid: 真的是太垃圾了，，，，，，，，，，，，，，，，，，，，，，，，，， | down 简直是太垃圾了，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "origin: 靠 哥们算是被你们 <UNK>爹了 | top: 很漂亮 和图片上一样 物流也很快 | mid: 跟图片上的完全不一样 | down 垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾\n",
      "origin: 难看差评难看差评难看差评 | top: 很漂亮的戒指，款式简单大方，戴上很好看 | mid: 太假了。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | down 太假了。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 掉色严重，两天就花了，不建议购买 | top: 宝贝收到了，很漂亮，很喜欢，以后还会光顾的 | mid: 宝贝收到了，和图片上的不一样 | down 货收到了，和图片上的不一样\n",
      "origin: 不满意，态度不行。 | top: 很漂亮，做工也很精致。 | mid: 一般般。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | down 和图片不一样\n",
      "origin: 啥东西。 不好 上当了 | top: 非常满意，物流很快，客服态度也很好 | mid: 差评，物流太慢了！ | down 差评，太垃圾了\n",
      "origin: 买的是女款 却给了个男款 | top: 还可以吧！！！！！！！！！！！！！！！！！！！！！！！！！！！！ | mid: 还可以吧！！！！！！！！！！！！！！！！！！！！！！！！！！！！ | down 货都没收到 没几天就掉色了\n",
      "origin: 快递太慢了，一份价钱一份货吧 | top: 还可以吧。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | mid: 还可以吧。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | down 太小了。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 这个一般！ | top: 这个好用 | mid: 真心觉得不值这个价 | down 质量太差\n",
      "origin: 好，非常好看，有<UNK><UNK> | top: 挺好的，就是物流有点慢，三四天才到 | mid: 不好，一点也不好，上面的樱桃就掉了 | down 不好，一点也不好，轻轻一弄就断了\n",
      "origin: 没用就不见了 | top: 还可以吧 | mid: 还可以吧 | down 没几天就断了\n",
      "origin: 一般吧，只是一面水晶，后面<UNK>了不太喜欢 | top: 和图片上的一样，就是物流有点慢，不过还是很不错的 | mid: 没有想象中的那么好，不过这个价钱也就这样了 | down 和图片上的不一样，而且还有一个洞，感觉被坑了\n",
      "origin: 链子不怎么样，连个首饰盒都没有。 | top: 宝贝收到了，质量很好，非常喜欢 | mid: 质量太差，没几天就断了 | down 质量太差，后悔死了\n",
      "origin: 质量还可以，还送了小礼物。上面的<UNK>可能<UNK>。 | top: 宝贝收到了，质量很好，做工也很精致，色泽也不错，很满意的一次购物。 | mid: 总体来说还不错，就是链子有点细。不过还是挺好的。。。。。。。。。。。。。。。。 | down 东西还可以，但是物流太慢了八天才到。东西还可以。。。。。。。。。。。。。。。。\n",
      "origin: ……<UNK> | top: 凑单买的。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | mid: 一分钱一分货。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | down 无语。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 物流很难 | top: 一大包 | mid: 一搬搬 | down 一搬搬\n",
      "origin: 下个<UNK> | top: 嗯嗯嗯嗯嗯嗯嗯嗯嗯嗯嗯嗯嗯 | mid: <UNK> | down <UNK><UNK><UNK><UNK>\n",
      "INFO:tensorflow:Restoring parameters from /root/VAE/char_vae_model/runs/1513646690/model-15\n",
      "restore /root/VAE/char_vae_model/runs/1513646690/model-15 success\n",
      "generate top:\n",
      "【 收到了，包装的很好，款式漂亮，做工精致，价格实惠，很喜欢！ 】\n",
      "【 很好看，物流也很快 】\n",
      "【 宝贝收到了，很漂亮的一款项链，做工也很精致，很满意的一次购物。 】\n",
      "\n",
      "generate mid:\n",
      "【 不满意的一次购物，物流太慢了 】\n",
      "【 很满意的一次购物，很喜欢 】\n",
      "【 还可以，就是链子太细了 】\n",
      "\n",
      "generate down:\n",
      "【 太难看了！！！！！！！！！！！！！！！！！！！！！！！！！！！！ 】\n",
      "【 东西很一般，和图片上的不一样，而且物流也很慢，建议大家不要买。 】\n",
      "【 <UNK> 】\n",
      "\n",
      "origin: 货收到了，我很满意。手镯很好。下次还会来的。 | top: 东西很好，还送了小礼物，谢谢店家送的小礼物 | mid: 东西还行，就是说<UNK>的<UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK> | down 我没有收到货的时候就交易完成了\n",
      "origin: 很不错的一款项链，非常满意。 | top: 很好的卖家，发货速度也很快。 | mid: 和想象中的不一样 | down 垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾\n",
      "origin: 东西很好 就是有点大啊 | top: 东西很好，我老婆很喜欢，就是物流有点慢 | mid: 不是很好，我觉得不值这个价 | down 我觉得不值这个价，<UNK>就好了\n",
      "origin: 收到宝贝。看着不错也很漂亮。 | top: 东西不错，老婆很喜欢。 | mid: 帮朋友买的，朋友说还不错。 | down 一星都不想给。\n",
      "origin: 看着价位很不值，戴上还可以。 | top: 还可以，就是链子有点细。。。。。。。。。。。。。。。。。。。。。。。。 | mid: 还可以，就是链子有点细。。。。。。。。。。。。。。。。。。。。。。。。 | down 不好看。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 质量太垃圾了 | top: 东西收到了，非常满意 | mid: 东西收到了 | down 太差了\n",
      "origin: 相当给力，店主送的宝物俺很喜欢！！！将继续关注啊。 | top: 还可以吧！！！！！！！！！！！！！！！！！！！！！！！！！！！！ | mid: 还可以吧！！！！！！！！！！！！！！！！！！！！！！！！！！！！ | down 说好的赠品呢？？？？？？？？？？？？？？？？？？？？？？？？？？？\n",
      "origin: 项链很漂亮，做工精致光泽度好，喜欢 | top: 还可以吧！！！！！！！！！！！！！！！！！！！！！！！！！！！！ | mid: 还可以吧！！！！！！！！！！！！！！！！！！！！！！！！！！！！ | down 还可以吧！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
      "origin: 夹子很好夹起来也很好看下次会再买 | top: 很好很好很好很好很好很好很好很好很好很好很好很好很好很好很好很 | mid: 很一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般一般 | down 不是很好用\n",
      "origin: 很满意的说 | top: 质量很好，很满意的一次购物 | mid: 还行，满意 | down 差差差差差差差差差差差差差差差差差差差差差差差差差差差差差差差差差\n",
      "origin: 这个买回来真心好丑，<UNK>还是错的，下次不买了 | top: 虽然是赠品，但还是不错的！ | mid: 虽然是赠品，但还是挺好的！ | down 虽然是赠品，但是质量太差了！！！！！！！！！！！！！！！！！！！！！！！\n",
      "origin: 项链和描述的一样，但是 快递小弟很讨厌 | top: 东西很好，快递也很快，很满意的一次购物 | mid: 东西还行，就是快递太慢了！ | down 东西还行，就是快递太慢了！！！！！！！！！！！！！！！！！！！！！！！\n",
      "origin: ？？？？？？？假的，真是一分钱一分货，一点也没错 | top: 很好看，价格也不贵，性价比高，很满意的一次购物。 | mid: 不咋地，，，，，，，，，，，，，，，，，，，，，，，，，，，， | down ***的东西，，，，，，，，，，，，，，，，，，，，，，，，，，\n",
      "origin: <UNK>们，被骗了，太垃圾了 | top: 很好，和图片上的一样，很喜欢。 | mid: 和照片上的不太一样，太容易断了。 | down 一点都不好，又不对，又不好看。\n",
      "origin: 靠 哥们算是被你们 <UNK>爹了 | top: 不错 快递很快 前天下单今天中午就到了 | mid: 呵呵呵呵呵呵呵呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃呃 | down 垃圾 刚买来就坏了\n",
      "origin: 难看差评难看差评难看差评 | top: 很漂亮的一款项链，做工精致，卖家服务很好 | mid: 跟图片上的完全不一样 | down 垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾垃圾\n",
      "origin: 掉色严重，两天就花了，不建议购买 | top: 收到了，很漂亮，做工精细，戴上很好看，很喜欢 | mid: 太小了，带起来不舒服，不好看 | down 太垃圾了，刚到手就断了\n",
      "origin: 不满意，态度不行。 | top: 质量不错，做工精致，很喜欢 | mid: 还可以，就是链子太细了 | down 还可以，就是链子太细了\n",
      "origin: 啥东西。 不好 上当了 | top: 东西很好，物流也很快，很满意的一次购物 | mid: 东西很好，就是物流有点慢 | down 一点都不好，<UNK>\n",
      "origin: 买的是女款 却给了个男款 | top: 送朋友的，朋友说还可以吧！ | mid: 买的时候还可以，就是不知道是不是银的 | down 给朋友买的，还没打开就坏了\n",
      "origin: 快递太慢了，一份价钱一份货吧 | top: 一份价钱一份钱一分货，只能说还行吧 | mid: 一份价钱一份货，只能说一分钱一分货。 | down 一份价钱一份货，只能当摆设了。\n",
      "origin: 这个一般！ | top: 东西收到了，质量很好，物流也很快 | mid: 东西还行，就是物流太慢了 | down 东西还行，就是物流太慢了\n",
      "origin: 好，非常好看，有<UNK><UNK> | top: 很漂亮，做工很精致，包装也很好，非常满意的一次购物。 | mid: 。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | down 垃圾。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: 没用就不见了 | top: 还没用，看着还不错 | mid: 还行吧，就是有点小了 | down 没用多久就断了\n",
      "origin: 一般吧，只是一面水晶，后面<UNK>了不太喜欢 | top: 挺好的，就是有点小，可能是我手粗的原因。 | mid: 还可以，就是有点小，可能是我胳膊粗的缘故。 | down 还可以吧，就是有点小，和图片上的不一样\n",
      "origin: 链子不怎么样，连个首饰盒都没有。 | top: 宝贝收到了，紫罗兰色水晶很闪很漂亮，很喜欢。 | mid: 实物和图片上的不一样 | down 实物和图片上的不一样\n",
      "origin: 质量还可以，还送了小礼物。上面的<UNK>可能<UNK>。 | top: 东西很好，就是物流有点慢。。。。。。。。。。。。。。。。。。。。。。。 | mid: 东西还行，就是物流有点慢。。。。。。。。。。。。。。。。。。。。。。。 | down 一点都不好。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "origin: ……<UNK> | top: <UNK><UNK><UNK><UNK><UNK> | mid: <UNK><UNK><UNK><UNK> | down 带了一天就掉漆了\n",
      "origin: 物流很难 | top: 物流挺快的，东西也不错 | mid: 还可以吧。。。。。。。。。。。。。。。。。。。。。。。。。。。。 | down 物流太慢，东西还行\n",
      "origin: 下个<UNK> | top: <UNK> | mid: <UNK> | down <UNK>\n"
     ]
    }
   ],
   "source": [
    "for i in range(12, 15):\n",
    "    model.restore('/root/VAE/char_vae_model/runs/1513646690/model-%d' % i)\n",
    "    generate()\n",
    "    for s in candi:\n",
    "        senti_transfer(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-20T01:54:36.244165Z",
     "start_time": "2017-12-20T01:54:36.151154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin: 货收到了，我很满意。手镯很好。下次还会来的。\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 16) for Tensor u'concat_1:0', which has shape '(?, 20)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7513d65af00e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'origin:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msenti_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-bbc02da2456a>\u001b[0m in \u001b[0;36msenti_transfer\u001b[0;34m(A, cls)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msenti_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxToz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzTox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0moo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0moo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c6f9c017414d>\u001b[0m in \u001b[0;36mzTox\u001b[0;34m(self, z, c)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mzTox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         out_indices = self.sess.run(self.predicting_ids, {self.batch_size:z.shape[0],\n\u001b[0;32m--> 298\u001b[0;31m             self.z:z, self.C:[c], self.output_keep_prob:1, self.input_keep_prob:1})\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    973\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    976\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 16) for Tensor u'concat_1:0', which has shape '(?, 20)'"
     ]
    }
   ],
   "source": [
    "for z in candi[:10]:\n",
    "    print 'origin:', z\n",
    "    senti_transfer(z, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-19T01:24:06.273Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
