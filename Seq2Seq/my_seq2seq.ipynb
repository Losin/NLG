{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-13T01:06:31.845104Z",
     "start_time": "2017-12-13T01:06:29.124295Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from tensorflow.python.layers import core as core_layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import myResidualCell\n",
    "import jieba\n",
    "from bleu import BLEU\n",
    "import random\n",
    "import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_initializer(matrix):\n",
    "    def _initializer(shape, dtype=None, partition_info=None, **kwargs): return matrix\n",
    "    return _initializer\n",
    "\n",
    "class Seq2Seq:\n",
    "    def __init__(self, dp, rnn_size, n_layers, encoder_embedding_dim, decoder_embedding_dim, max_infer_length,\n",
    "                 sess=tf.Session(), lr=0.0001, grad_clip=5.0, beam_width=5, force_teaching_ratio=1.0, beam_penalty=1.0,\n",
    "                residual=False, output_keep_prob=0.5, input_keep_prob=0.9, cell_type='lstm', reverse=False,\n",
    "                encoder_pre_embedding=None, decoder_pre_embedding=None, decay_scheme='luong234', is_save=True, emb_fix=False):\n",
    "        \n",
    "        self.rnn_size = rnn_size\n",
    "        self.n_layers = n_layers\n",
    "        self.grad_clip = grad_clip\n",
    "        self.dp = dp\n",
    "        self.encoder_embedding_dim = encoder_embedding_dim\n",
    "        self.decoder_embedding_dim = decoder_embedding_dim\n",
    "        self.encoder_pre_embedding = encoder_pre_embedding\n",
    "        self.decoder_pre_embedding = decoder_pre_embedding\n",
    "        self.beam_width = beam_width\n",
    "        self.beam_penalty = beam_penalty\n",
    "        self.max_infer_length = max_infer_length\n",
    "        self.residual = residual\n",
    "        self.decay_scheme = decay_scheme\n",
    "        if self.residual:\n",
    "            assert encoder_embedding_dim == rnn_size\n",
    "            assert decoder_embedding_dim == rnn_size\n",
    "        self.reverse = reverse\n",
    "        self.emb_fix = emb_fix\n",
    "        self.cell_type = cell_type\n",
    "        self.force_teaching_ratio = force_teaching_ratio\n",
    "        self._output_keep_prob = output_keep_prob\n",
    "        self._input_keep_prob = input_keep_prob\n",
    "        self.sess = sess\n",
    "        self.lr=lr\n",
    "        self.build_graph()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep = 5)\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n",
    "        self.is_save = is_save\n",
    "        \n",
    "    # end constructor\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.register_symbols()\n",
    "        self.add_input_layer()\n",
    "        self.add_encoder_layer()\n",
    "        with tf.variable_scope('decode'):\n",
    "            self.add_decoder_for_training()\n",
    "        with tf.variable_scope('decode', reuse=True):\n",
    "            self.add_decoder_for_inference()\n",
    "        #with tf.variable_scope('decode', reuse=True):\n",
    "        #    self.add_decoder_for_prefix_inference()\n",
    "        #with tf.variable_scope('decode', reuse=True):\n",
    "        #    self.add_decoder_for_greedy_inference()\n",
    "        self.add_backward_path()\n",
    "    # end method\n",
    "\n",
    "    def add_input_layer(self):\n",
    "        self.X = tf.placeholder(tf.int32, [None, None], name=\"X\")\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None], name=\"Y\")\n",
    "        self.X_seq_len = tf.placeholder(tf.int32, [None], name=\"X_seq_len\")\n",
    "        self.Y_seq_len = tf.placeholder(tf.int32, [None], name=\"Y_seq_len\")\n",
    "        self.input_keep_prob = tf.placeholder(tf.float32,name=\"input_keep_prob\")\n",
    "        self.output_keep_prob = tf.placeholder(tf.float32,name=\"output_keep_prob\")\n",
    "        self.batch_size = tf.shape(self.X)[0]\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    # end method\n",
    "\n",
    "    def single_cell(self, reuse=False):\n",
    "        if self.cell_type == 'lstm':\n",
    "             cell = tf.contrib.rnn.LayerNormBasicLSTMCell(self.rnn_size, reuse=reuse)\n",
    "        else:\n",
    "            cell = tf.contrib.rnn.GRUBlockCell(self.rnn_size)    \n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, self.output_keep_prob, self.input_keep_prob)\n",
    "        if self.residual:\n",
    "            cell = myResidualCell.ResidualWrapper(cell)\n",
    "        return cell\n",
    "    \n",
    "    def add_encoder_layer(self):\n",
    "        if type(self.encoder_pre_embedding) != type(None):\n",
    "            if self.emb_fix:\n",
    "                encoder_embedding = tf.get_variable('encoder_embedding', [len(self.dp.X_w2id), self.encoder_embedding_dim],\n",
    "                                                     tf.float32, initializer=get_initializer(self.encoder_pre_embedding), trainable=False) \n",
    "            else:    \n",
    "                encoder_embedding = tf.get_variable('encoder_embedding', [len(self.dp.X_w2id), self.encoder_embedding_dim],\n",
    "                                                     tf.float32, initializer=get_initializer(self.encoder_pre_embedding)) \n",
    "        else:\n",
    "            encoder_embedding = tf.get_variable('encoder_embedding', [len(self.dp.X_w2id), self.encoder_embedding_dim],\n",
    "                                                 tf.float32, tf.random_uniform_initializer(-1.0, 1.0))\n",
    "        \n",
    "        self.encoder_inputs = tf.nn.embedding_lookup(encoder_embedding, self.X)\n",
    "        bi_encoder_output, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw = tf.contrib.rnn.MultiRNNCell([self.single_cell() for _ in range(self.n_layers)]), \n",
    "            cell_bw = tf.contrib.rnn.MultiRNNCell([self.single_cell() for _ in range(self.n_layers)]),\n",
    "            inputs = self.encoder_inputs,\n",
    "            sequence_length = self.X_seq_len,\n",
    "            dtype = tf.float32,\n",
    "            scope = 'bidirectional_rnn')\n",
    "        self.encoder_out = tf.concat(bi_encoder_output, 2)\n",
    "        encoder_state = []\n",
    "        for layer_id in range(self.n_layers):\n",
    "            encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
    "            encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
    "        self.encoder_state = tuple(encoder_state)\n",
    "\n",
    "    def processed_decoder_input(self):\n",
    "        main = tf.strided_slice(self.Y, [0, 0], [self.batch_size, -1], [1, 1]) # remove last char\n",
    "        decoder_input = tf.concat([tf.fill([self.batch_size, 1], self._y_go), main], 1)\n",
    "        return decoder_input\n",
    "\n",
    "    def add_attention_for_training(self):\n",
    "        if self.cell_type == 'lstm':\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units = self.rnn_size, \n",
    "                memory = self.encoder_out,\n",
    "                memory_sequence_length = self.X_seq_len,\n",
    "                normalize=True)\n",
    "        else:\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                num_units = self.rnn_size, \n",
    "                memory = self.encoder_out,\n",
    "                memory_sequence_length = self.X_seq_len,\n",
    "                scale=True)\n",
    "        \n",
    "        self.decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([self.single_cell() for _ in range(2 * self.n_layers)]),\n",
    "            attention_mechanism = attention_mechanism,\n",
    "            alignment_history = True,\n",
    "            attention_layer_size = self.rnn_size)\n",
    "\n",
    "    def add_decoder_for_training(self):\n",
    "        self.add_attention_for_training()\n",
    "        if type(self.decoder_pre_embedding) != type(None):\n",
    "            if self.emb_fix:\n",
    "                decoder_embedding = tf.get_variable('decoder_embedding', [len(self.dp.Y_w2id), self.decoder_embedding_dim],\n",
    "                                                     tf.float32, initializer=get_initializer(self.decoder_pre_embedding), trainable=False)\n",
    "            else:\n",
    "                decoder_embedding = tf.get_variable('decoder_embedding', [len(self.dp.Y_w2id), self.decoder_embedding_dim],\n",
    "                                                     tf.float32, initializer=get_initializer(self.decoder_pre_embedding)) \n",
    "        else:\n",
    "            decoder_embedding = tf.get_variable('decoder_embedding', [len(self.dp.Y_w2id), self.decoder_embedding_dim],\n",
    "                                                 tf.float32, tf.random_uniform_initializer(-1.0, 1.0))\n",
    "        training_helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "            inputs = tf.nn.embedding_lookup(decoder_embedding, self.processed_decoder_input()),\n",
    "            sequence_length = self.Y_seq_len,\n",
    "            embedding = decoder_embedding,\n",
    "            sampling_probability = 1 - self.force_teaching_ratio,\n",
    "            time_major = False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell = self.decoder_cell,\n",
    "            helper = training_helper,\n",
    "            initial_state = self.decoder_cell.zero_state(self.batch_size, tf.float32).clone(cell_state=self.encoder_state),\n",
    "            output_layer = core_layers.Dense(len(self.dp.Y_w2id)))\n",
    "        training_decoder_output, self.train_final_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder = training_decoder,\n",
    "            impute_finished = True,\n",
    "            maximum_iterations = tf.reduce_max(self.Y_seq_len))\n",
    "        self.training_logits = training_decoder_output.rnn_output\n",
    "        self.init_prefix_state = self.train_final_state\n",
    "\n",
    "    def add_attention_for_inference(self):\n",
    "        self.encoder_out_tiled = tf.contrib.seq2seq.tile_batch(self.encoder_out, self.beam_width)\n",
    "        self.encoder_state_tiled = tf.contrib.seq2seq.tile_batch(self.encoder_state, self.beam_width)\n",
    "        self.X_seq_len_tiled = tf.contrib.seq2seq.tile_batch(self.X_seq_len, self.beam_width)\n",
    "        if self.cell_type == 'lstm':\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units = self.rnn_size, \n",
    "                memory = self.encoder_out_tiled,\n",
    "                memory_sequence_length = self.X_seq_len_tiled,\n",
    "                normalize=True)\n",
    "        else:\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                num_units = self.rnn_size, \n",
    "                memory = self.encoder_out_tiled,\n",
    "                memory_sequence_length = self.X_seq_len_tiled,\n",
    "                scale=True)\n",
    "        self.decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([self.single_cell(reuse=True) for _ in range(2 * self.n_layers)]),\n",
    "            attention_mechanism = attention_mechanism,\n",
    "            attention_layer_size = self.rnn_size)\n",
    "        \n",
    "    def add_attention_for_greedy_inference(self):\n",
    "        if self.cell_type == 'lstm':\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units = self.rnn_size, \n",
    "                memory = self.encoder_out,\n",
    "                memory_sequence_length = self.X_seq_len,\n",
    "                normalize=True)\n",
    "        else:\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                num_units = self.rnn_size, \n",
    "                memory = self.encoder_out,\n",
    "                memory_sequence_length = self.X_seq_len,\n",
    "                scale=True)\n",
    "        \n",
    "        self.decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([self.single_cell() for _ in range(2 * self.n_layers)]),\n",
    "            attention_mechanism = attention_mechanism,\n",
    "            alignment_history = True,\n",
    "            attention_layer_size = self.rnn_size)\n",
    "        \n",
    "    def add_decoder_for_greedy_inference(self):\n",
    "        self.add_attention_for_greedy_inference()\n",
    "        decoder_embedding = tf.get_variable('decoder_embedding')\n",
    "        greedy_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding= decoder_embedding, \n",
    "            start_tokens = tf.tile(tf.constant([self._y_go], dtype=tf.int32), [self.batch_size]), \n",
    "            end_token = self._y_eos)\n",
    "        greedy_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell = self.decoder_cell,\n",
    "            helper = greedy_helper,\n",
    "            initial_state = self.decoder_cell.zero_state(self.batch_size, tf.float32).clone(cell_state=self.encoder_state),\n",
    "            output_layer = core_layers.Dense(len(self.dp.Y_w2id)))\n",
    "        greedy_decoder_output, self.greedy_final_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder = greedy_decoder,\n",
    "            impute_finished = False,\n",
    "            maximum_iterations = self.max_infer_length)\n",
    "       \n",
    "        self.greedy_output = greedy_decoder_output.sample_id  \n",
    "        self.alignment_history = self.greedy_final_state.alignment_history.stack()\n",
    "        \n",
    "    def add_decoder_for_inference(self):\n",
    "        self.add_attention_for_inference()\n",
    "        ini = self.decoder_cell.zero_state(self.batch_size * self.beam_width, tf.float32).clone(\n",
    "                            cell_state = self.encoder_state_tiled)\n",
    "        predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "            cell = self.decoder_cell,\n",
    "            embedding = tf.get_variable('decoder_embedding'),\n",
    "            start_tokens = tf.tile(tf.constant([self._y_go], dtype=tf.int32), [self.batch_size]),\n",
    "            end_token = self._y_eos,\n",
    "            initial_state = self.decoder_cell.zero_state(self.batch_size * self.beam_width, tf.float32).clone(\n",
    "                            cell_state = self.encoder_state_tiled),\n",
    "            beam_width = self.beam_width,\n",
    "            output_layer = core_layers.Dense(len(self.dp.Y_w2id), _reuse=True),\n",
    "            length_penalty_weight = self.beam_penalty)\n",
    "        predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder = predicting_decoder,\n",
    "            impute_finished = False,\n",
    "            maximum_iterations = self.max_infer_length)\n",
    "        self.predicting_ids = predicting_decoder_output.predicted_ids\n",
    "        self.score = predicting_decoder_output.beam_search_decoder_output.scores\n",
    "        \n",
    "    def add_decoder_for_prefix_inference(self):\n",
    "        self.add_attention_for_inference()\n",
    "\n",
    "        prefix_cell_state = tf.contrib.seq2seq.tile_batch(self.init_prefix_state.cell_state, self.beam_width)\n",
    "        prefix_attention = tf.contrib.seq2seq.tile_batch(self.init_prefix_state.attention, self.beam_width)\n",
    "        prefix_time = self.init_prefix_state.time\n",
    "        prefix_alignments = self.init_prefix_state.alignments\n",
    "        prefix_alignment_history = self.init_prefix_state.alignment_history\n",
    "        \n",
    "        init_state = tf.contrib.seq2seq.AttentionWrapperState(cell_state=prefix_cell_state, \n",
    "                                                      attention=prefix_attention, \n",
    "                                                      time=prefix_time,\n",
    "                                                      alignments=prefix_alignments,\n",
    "                                                      alignment_history=prefix_alignment_history)\n",
    "        predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "            cell = self.decoder_cell,\n",
    "            embedding = tf.get_variable('decoder_embedding'),\n",
    "            start_tokens = tf.tile(tf.constant([self._y_go], dtype=tf.int32), [self.batch_size]),\n",
    "            end_token = self._y_eos,\n",
    "            initial_state = init_state,\n",
    "            beam_width = self.beam_width,\n",
    "            output_layer = core_layers.Dense(len(self.dp.Y_w2id), _reuse=True),\n",
    "            length_penalty_weight = self.beam_penalty)\n",
    "        self.prefix_go = tf.placeholder(tf.int32, [None])\n",
    "        prefix_go_beam = tf.tile(tf.expand_dims(self.prefix_go, 1), [1, self.beam_width])\n",
    "        prefix_emb = tf.nn.embedding_lookup(tf.get_variable('decoder_embedding'), prefix_go_beam)\n",
    "        predicting_decoder._start_inputs = prefix_emb\n",
    "        predicting_prefix_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder = predicting_decoder,\n",
    "            impute_finished = False,\n",
    "            maximum_iterations = self.max_infer_length)\n",
    "        self.predicting_prefix_ids = predicting_prefix_decoder_output.predicted_ids\n",
    "        self.prefix_score = predicting_prefix_decoder_output.beam_search_decoder_output.scores\n",
    "\n",
    "    def add_backward_path(self):\n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        self.loss = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
    "                                                     targets = self.Y,\n",
    "                                                     weights = masks)\n",
    "        self.batch_loss = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
    "                                                     targets = self.Y,\n",
    "                                                     weights = masks,\n",
    "                                                     average_across_batch=False)\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(self.loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.grad_clip)\n",
    "        self.learning_rate = tf.constant(self.lr)\n",
    "        self.learning_rate = self.get_learning_rate_decay(self.decay_scheme)  # decay\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n",
    "\n",
    "    def register_symbols(self):\n",
    "        self._x_go = self.dp.X_w2id['<GO>']\n",
    "        self._x_eos = self.dp.X_w2id['<EOS>']\n",
    "        self._x_pad = self.dp.X_w2id['<PAD>']\n",
    "        self._x_unk = self.dp.X_w2id['<UNK>']\n",
    "        \n",
    "        self._y_go = self.dp.Y_w2id['<GO>']\n",
    "        self._y_eos = self.dp.Y_w2id['<EOS>']\n",
    "        self._y_pad = self.dp.Y_w2id['<PAD>']\n",
    "        self._y_unk = self.dp.Y_w2id['<UNK>']\n",
    "    \n",
    "    def infer(self, input_word):\n",
    "        input_indices = [self.dp.X_w2id.get(char, self._x_unk) for char in input_word]\n",
    "        out_indices = self.sess.run(self.predicting_ids, {\n",
    "            self.X: [input_indices], self.X_seq_len: [len(input_indices)], self.output_keep_prob:1, self.input_keep_prob:1})\n",
    "        outputs = []\n",
    "        for idx in range(out_indices.shape[-1]):\n",
    "            eos_id = self.dp.Y_w2id['<EOS>']\n",
    "            ot = out_indices[0,:,idx]\n",
    "            if eos_id in ot:\n",
    "                ot = ot.tolist()\n",
    "                ot = ot[:ot.index(eos_id)]\n",
    "            if self.reverse:\n",
    "                ot = ot[::-1]\n",
    "            output_str = ''.join([self.dp.Y_id2w.get(i, u'&') for i in ot])\n",
    "            outputs.append(output_str)\n",
    "        return outputs\n",
    "    \n",
    "    def batch_infer(self, input_words):\n",
    "        input_indices = [[self.dp.X_w2id.get(char, self._x_unk) for char in input_word] for input_word in input_words]\n",
    "        input_indices, lengths = self.dp.pad_sentence_batch(input_indices, self._x_pad)\n",
    "        out_indices = self.sess.run(self.predicting_ids, {\n",
    "            self.X: input_indices, self.X_seq_len: lengths, self.output_keep_prob:1, self.input_keep_prob:1})\n",
    "        outputs = []\n",
    "        for idx in range(out_indices.shape[0]):\n",
    "            eos_id = self.dp.Y_w2id['<EOS>']\n",
    "            ot = out_indices[idx,:,0]   # (batch, length, beam)\n",
    "            if eos_id in ot: \n",
    "                ot = ot.tolist()\n",
    "                ot = ot[:ot.index(eos_id)]\n",
    "            if self.reverse:\n",
    "                ot = ot[::-1]\n",
    "            output_str = ''.join([self.dp.Y_id2w.get(i, u'&') for i in ot])\n",
    "            outputs.append(output_str)\n",
    "        assert len(outputs) == len(input_words)\n",
    "        return outputs\n",
    "    \n",
    "    def prefix_infer(self, input_word, prefix):\n",
    "        input_indices_X = [self.dp.X_w2id.get(char, self._x_unk) for char in input_word]\n",
    "        input_indices_Y = [self.dp.Y_w2id.get(char, self._y_unk) for char in prefix]\n",
    "        \n",
    "        prefix_go = []\n",
    "        prefix_go.append(input_indices_Y[-1]) \n",
    "        out_indices, scores = self.sess.run([self.predicting_prefix_ids, self.prefix_score], {\n",
    "            self.X: [input_indices_X], self.X_seq_len: [len(input_indices_X)], self.Y:[input_indices_Y], self.Y_seq_len:[len(input_indices_Y)],\n",
    "            self.prefix_go: prefix_go, self.input_keep_prob:1, self.output_keep_prob:1})\n",
    "        \n",
    "        outputs = []\n",
    "        for idx in range(out_indices.shape[-1]):\n",
    "            eos_id = self.dp.Y_w2id['<EOS>']\n",
    "            ot = out_indices[0,:,idx]\n",
    "            if eos_id in ot:\n",
    "                ot = ot.tolist()\n",
    "                ot = ot[:ot.index(eos_id)]\n",
    "                if self.reverse:\n",
    "                    ot = ot[::-1]\n",
    "            if self.reverse:\n",
    "                output_str = ''.join([self.dp.Y_id2w.get(i, u'&') for i in ot]) + prefix\n",
    "            else:\n",
    "                output_str = prefix + ''.join([self.dp.Y_id2w.get(i, u'&') for i in ot])\n",
    "            outputs.append(output_str)\n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "    def restore(self, path):\n",
    "        self.saver.restore(self.sess, path)\n",
    "        print 'restore %s success' % path\n",
    "        \n",
    "    def get_learning_rate_decay(self, decay_scheme='luong234'):\n",
    "        num_train_steps = self.dp.num_steps\n",
    "        if decay_scheme == \"luong10\":\n",
    "            start_decay_step = int(num_train_steps / 2)\n",
    "            remain_steps = num_train_steps - start_decay_step\n",
    "            decay_steps = int(remain_steps / 10)  # decay 10 times\n",
    "            decay_factor = 0.5\n",
    "        else:\n",
    "            start_decay_step = int(num_train_steps * 2 / 3)\n",
    "            remain_steps = num_train_steps - start_decay_step\n",
    "            decay_steps = int(remain_steps / 4)  # decay 4 times\n",
    "            decay_factor = 0.5\n",
    "        return tf.cond(\n",
    "            self.global_step < start_decay_step,\n",
    "            lambda: self.learning_rate,\n",
    "            lambda: tf.train.exponential_decay(\n",
    "                self.learning_rate,\n",
    "                (self.global_step - start_decay_step),\n",
    "                decay_steps, decay_factor, staircase=True),\n",
    "            name=\"learning_rate_decay_cond\")\n",
    "    \n",
    "    def setup_summary(self):\n",
    "        train_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar('Train_loss', train_loss)\n",
    "        \n",
    "        test_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar('Test_loss', test_loss)\n",
    "        \n",
    "        bleu_score = tf.Variable(0.)\n",
    "        tf.summary.scalar('BLEU_score', bleu_score)\n",
    "\n",
    "        tf.summary.scalar('lr_rate', self.learning_rate)\n",
    "        \n",
    "        summary_vars = [train_loss, test_loss, bleu_score]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in xrange(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in xrange(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op\n",
    "    \n",
    "    def add_visual_summary(self):\n",
    "        \"\"\"create attention image and attention summary.\"\"\"\n",
    "        self.attention_alignment = (self.train_final_state.alignment_history.stack())\n",
    "        # Reshape to (batch, src_seq_len, tgt_seq_len,1)\n",
    "        attention_images = tf.expand_dims(\n",
    "              tf.transpose(self.attention_alignment, [1, 2, 0]), -1)\n",
    "        # Scale to range [0, 255]\n",
    "        attention_images *= 255\n",
    "        self.attention_summary = tf.summary.image(\"attention_images\", attention_images)\n",
    "        \n",
    "    def write_attention_summary(self, summary_writer, X, Y):\n",
    "        if self.reverse:\n",
    "            Y = Y[::-1]\n",
    "        input_indices = [self.dp.X_w2id.get(char, self._x_unk) for char in X]\n",
    "        output_indices = [self.dp.Y_w2id.get(char, self._x_unk) for char in Y]\n",
    "\n",
    "        summary_str, attention_alignment = self.sess.run([self.attention_summary, self.attention_alignment], {self.X: [input_indices],\n",
    "                                                     self.Y: [output_indices],\n",
    "                                                     self.X_seq_len: [len(input_indices)],\n",
    "                                                     self.Y_seq_len: [len(output_indices)],\n",
    "                                                     self.output_keep_prob:1,\n",
    "                                                     self.input_keep_prob:1})\n",
    "        #print 'write summary'\n",
    "        summary_writer.add_summary(summary_str, 1)\n",
    "        return attention_alignment\n",
    "    \n",
    "    def show_attention(self, X, Y, is_show=True):\n",
    "        if self.reverse:\n",
    "            Y = Y[::-1]\n",
    "        input_indices = [self.dp.X_w2id.get(char, self._x_unk) for char in X]\n",
    "        output_indices = [self.dp.Y_w2id.get(char, self._x_unk) for char in Y]\n",
    "\n",
    "        attention_alignment = self.sess.run(self.attention_alignment, {self.X: [input_indices],\n",
    "                                                     self.Y: [output_indices],\n",
    "                                                     self.X_seq_len: [len(input_indices)],\n",
    "                                                     self.Y_seq_len: [len(output_indices)],\n",
    "                                                     self.output_keep_prob:1,\n",
    "                                                     self.input_keep_prob:1})\n",
    "        attention_alignment = attention_alignment.transpose((1,0,2))[0]  # (batch, tgt_seq_len, src_seq_len)\n",
    "        assert attention_alignment.shape[0] == len(Y)\n",
    "        assert attention_alignment.shape[1] == len(X)\n",
    "        if is_show:\n",
    "            for i,yw in enumerate(Y):\n",
    "                print yw, \" :\",\n",
    "                for j,x in enumerate(X):\n",
    "                    print \"%.2f%s\" % (attention_alignment[i][j], x),\n",
    "                print \"\"\n",
    "        return attention_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-13T01:06:31.888714Z",
     "start_time": "2017-12-13T01:06:31.846867Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Seq2Seq_DP:\n",
    "    def __init__(self, X_indices, Y_indices, X_w2id, Y_w2id, BATCH_SIZE, n_epoch):\n",
    "        assert len(X_indices) == len(Y_indices)\n",
    "        num_test = int(len(X_indices) * 0.1)\n",
    "        self.n_epoch = n_epoch\n",
    "        self.X_train = np.array(X_indices[num_test:])\n",
    "        self.Y_train = np.array(Y_indices[num_test:])\n",
    "        self.X_test = np.array(X_indices[:num_test])\n",
    "        self.Y_test = np.array(Y_indices[:num_test])\n",
    "        self.num_batch = int(len(self.X_train) / BATCH_SIZE)\n",
    "        self.num_steps = self.num_batch * self.n_epoch\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.X_w2id = X_w2id\n",
    "        self.X_id2w = dict(zip(X_w2id.values(), X_w2id.keys()))\n",
    "        self.Y_w2id = Y_w2id\n",
    "        self.Y_id2w = dict(zip(Y_w2id.values(), Y_w2id.keys()))\n",
    "        self._x_pad = self.X_w2id['<PAD>']\n",
    "        self._y_pad = self.Y_w2id['<PAD>']\n",
    "        print 'Train_data: %d | Test_data: %d | Batch_size: %d | Num_batch: %d | X_vocab_size: %d | Y_vocab_size: %d' % (len(self.X_train), len(self.X_test), BATCH_SIZE, self.num_batch, len(self.X_w2id), len(self.Y_w2id))\n",
    "        \n",
    "    def next_batch(self, X, Y):\n",
    "        r = np.random.permutation(len(X))\n",
    "        X = X[r]\n",
    "        Y = Y[r]\n",
    "        for i in range(0, len(X) - len(X) % self.batch_size, self.batch_size):\n",
    "            X_batch = X[i : i + self.batch_size]\n",
    "            Y_batch = Y[i : i + self.batch_size]\n",
    "            padded_X_batch, X_batch_lens = self.pad_sentence_batch(X_batch, self._x_pad)\n",
    "            padded_Y_batch, Y_batch_lens = self.pad_sentence_batch(Y_batch, self._y_pad)\n",
    "            yield (np.array(padded_X_batch),\n",
    "                   np.array(padded_Y_batch),\n",
    "                   X_batch_lens,\n",
    "                   Y_batch_lens)\n",
    "    \n",
    "    def sample_test_batch(self):\n",
    "        padded_X_batch, X_batch_lens = self.pad_sentence_batch(self.X_test[: self.batch_size], self._x_pad)\n",
    "        padded_Y_batch, Y_batch_lens = self.pad_sentence_batch(self.Y_test[: self.batch_size], self._y_pad)\n",
    "        return np.array(padded_X_batch), np.array(padded_Y_batch), X_batch_lens, Y_batch_lens\n",
    "        \n",
    "    def pad_sentence_batch(self, sentence_batch, pad_int):\n",
    "        padded_seqs = []\n",
    "        seq_lens = []\n",
    "        max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "        for sentence in sentence_batch:\n",
    "            padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "            seq_lens.append(len(sentence))\n",
    "        return padded_seqs, seq_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-13T01:06:32.198102Z",
     "start_time": "2017-12-13T01:06:31.891842Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Seq2Seq_util:\n",
    "    def __init__(self, dp, model, summary_path='Summary', display_freq=3):\n",
    "        self.display_freq = display_freq\n",
    "        self.dp = dp\n",
    "        self.model = model\n",
    "        self.summary_path = summary_path\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        avg_loss = 0.0\n",
    "        tic = time.time()\n",
    "        X_test_batch, Y_test_batch, X_test_batch_lens, Y_test_batch_lens = self.dp.sample_test_batch()\n",
    "        for local_step, (X_train_batch, Y_train_batch, X_train_batch_lens, Y_train_batch_lens) in enumerate(\n",
    "            self.dp.next_batch(self.dp.X_train, self.dp.Y_train)):\n",
    "            self.model.step, _, loss = self.model.sess.run([self.model.global_step, self.model.train_op, self.model.loss], \n",
    "                                          {self.model.X: X_train_batch,\n",
    "                                           self.model.Y: Y_train_batch,\n",
    "                                           self.model.X_seq_len: X_train_batch_lens,\n",
    "                                           self.model.Y_seq_len: Y_train_batch_lens,\n",
    "                                           self.model.output_keep_prob:self.model._output_keep_prob,\n",
    "                                           self.model.input_keep_prob:self.model._input_keep_prob})\n",
    "            avg_loss += loss\n",
    "            # summary\n",
    "            \"\"\"\n",
    "            if local_step % 10 == 0:\n",
    "                val_loss = self.model.sess.run(self.model.loss, {self.model.X: X_test_batch,\n",
    "                                                     self.model.Y: Y_test_batch,\n",
    "                                                     self.model.X_seq_len: X_test_batch_lens,\n",
    "                                                     self.model.Y_seq_len: Y_test_batch_lens,\n",
    "                                                     self.model.output_keep_prob:1,\n",
    "                                                     self.model.input_keep_prob:1})\n",
    "                stats = [avg_loss/(local_step+1), val_loss, 0]\n",
    "                for i in xrange(len(stats)):\n",
    "                    self.model.sess.run(self.model.update_ops[i], feed_dict={\n",
    "                        self.model.summary_placeholders[i]: float(stats[i])\n",
    "                    })\n",
    "                summary_str = self.model.sess.run(self.model.summary_op)\n",
    "                #print 'write summary'\n",
    "                self.summary_writer.add_summary(summary_str, self.model.step + 1)\n",
    "            \"\"\"\n",
    "            if local_step % (self.dp.num_batch / self.display_freq) == 0:\n",
    "                val_loss = self.model.sess.run(self.model.loss, {self.model.X: X_test_batch,\n",
    "                                                     self.model.Y: Y_test_batch,\n",
    "                                                     self.model.X_seq_len: X_test_batch_lens,\n",
    "                                                     self.model.Y_seq_len: Y_test_batch_lens,\n",
    "                                                     self.model.output_keep_prob:1,\n",
    "                                                     self.model.input_keep_prob:1})\n",
    "                print \"Epoch %d/%d | Batch %d/%d | Train_loss: %.3f | Test_loss: %.3f | Time_cost:%.3f\" % (epoch, self.n_epoch, local_step, self.dp.num_batch, avg_loss / (local_step + 1), val_loss, time.time()-tic)\n",
    "                self.cal()\n",
    "                tic = time.time()\n",
    "            \n",
    "        return avg_loss / self.dp.num_batch\n",
    "    \n",
    "    def test(self):\n",
    "        avg_loss = 0.0\n",
    "        for local_step, (X_test_batch, Y_test_batch, X_test_batch_lens, Y_test_batch_lens) in enumerate(\n",
    "            self.dp.next_batch(self.dp.X_test, self.dp.Y_test)):\n",
    "            val_loss = self.model.sess.run(self.model.loss, {self.model.X: X_test_batch,\n",
    "                                                 self.model.Y: Y_test_batch,\n",
    "                                                 self.model.X_seq_len: X_test_batch_lens,\n",
    "                                                 self.model.Y_seq_len: Y_test_batch_lens,\n",
    "                                                 self.model.output_keep_prob:1,\n",
    "                                                 self.model.input_keep_prob:1})\n",
    "            avg_loss += val_loss\n",
    "        return avg_loss / (local_step + 1)\n",
    "    \n",
    "    def fit(self, train_dir, is_bleu):\n",
    "        self.n_epoch = self.dp.n_epoch\n",
    "        test_loss_list = []\n",
    "        train_loss_list = []\n",
    "        time_cost_list = []\n",
    "        bleu_list = []\n",
    "        timestamp = str(int(time.time()))\n",
    "        #out_dir = os.path.abspath(os.path.join(train_dir, \"runs\", timestamp))\n",
    "        out_dir = os.path.join(train_dir, self.summary_path)\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        print \"Writing to %s\" % out_dir\n",
    "        checkpoint_prefix = os.path.join(out_dir, \"model\")\n",
    "        self.summary_writer = tf.summary.FileWriter(out_dir, self.model.sess.graph)\n",
    "        for epoch in range(1, self.n_epoch+1):\n",
    "            tic = time.time()\n",
    "            train_loss = self.train(epoch)\n",
    "            train_loss_list.append(train_loss)\n",
    "            test_loss = self.test()\n",
    "            test_loss_list.append(test_loss)\n",
    "            toc = time.time()\n",
    "            time_cost_list.append((toc - tic))\n",
    "            if is_bleu:\n",
    "                bleu = self.test_bleu()\n",
    "                bleu_list.append(bleu)\n",
    "                print \"Epoch %d/%d | Train_loss: %.3f | Test_loss: %.3f | Bleu: %.3f\" % (epoch, self.n_epoch, train_loss, test_loss, bleu)\n",
    "            else:\n",
    "                bleu = 0.0\n",
    "                print \"Epoch %d/%d | Train_loss: %.3f | Test_loss: %.3f\" % (epoch, self.n_epoch, train_loss, test_loss)\n",
    "            \n",
    "            stats = [train_loss, test_loss, bleu]\n",
    "            for i in xrange(len(stats)):\n",
    "                    self.model.sess.run(self.model.update_ops[i], feed_dict={\n",
    "                        self.model.summary_placeholders[i]: float(stats[i])\n",
    "                    })\n",
    "            summary_str = self.model.sess.run(self.model.summary_op)\n",
    "            #print 'write summary'\n",
    "            self.summary_writer.add_summary(summary_str, self.model.step + 1)\n",
    "            #print 'writing summary'\n",
    "            cPickle.dump((train_loss_list, test_loss_list, time_cost_list, bleu_list), open(os.path.join(out_dir,\"res.pkl\"),'wb'))\n",
    "            if self.model.is_save:    \n",
    "                path = self.model.saver.save(self.model.sess, checkpoint_prefix, global_step=epoch)\n",
    "                print \"Saved model checkpoint to %s\" % path\n",
    "    \n",
    "    def show(self, sent, id2w):\n",
    "        return \"\".join([id2w.get(idx, u'&') for idx in sent])\n",
    "    \n",
    "    def cal(self, n_example=5):\n",
    "        train_n_example = int(n_example / 2)\n",
    "        test_n_example = n_example - train_n_example\n",
    "        for _ in range(test_n_example):\n",
    "            example = self.show(self.dp.X_test[_], self.dp.X_id2w)\n",
    "            y = self.show(self.dp.Y_test[_], self.dp.Y_id2w)\n",
    "            o = self.model.infer(example)[0]\n",
    "            print 'TestInput: %s | Output: %s | GroundTruth: %s' % (example, o, y)\n",
    "        for _ in range(train_n_example):\n",
    "            example = self.show(self.dp.X_train[_], self.dp.X_id2w)\n",
    "            y = self.show(self.dp.Y_train[_], self.dp.Y_id2w)\n",
    "            o = self.model.infer(example)[0]\n",
    "            print 'TrainInput: %s | Output: %s | GroundTruth: %s' % (example, o, y) \n",
    "        print \"\"\n",
    "        \n",
    "    def test_bleu(self, gram=2, batch_size=256):\n",
    "        all_score = []\n",
    "        n_batch = int(len(self.dp.X_test) / batch_size)\n",
    "        for i in range(n_batch):\n",
    "            #tic = time.time()\n",
    "            input_indices = [self.show(X_test, self.dp.X_id2w) for X_test in self.dp.X_test[i*batch_size:(i+1)*batch_size]]\n",
    "            o = self.model.batch_infer(input_indices)\n",
    "            #print '%d batch_infer_time:%.3f' % (i, time.time() -tic)\n",
    "            #tic = time.time()\n",
    "            for j in range(batch_size):\n",
    "                if len(o[j]) < 1:\n",
    "                    all_score.append(0.0)\n",
    "                else:\n",
    "                    if self.model.reverse:\n",
    "                        refer4bleu = [[' '.join([self.dp.Y_id2w.get(w, u'&') for w in self.dp.Y_test[i*batch_size+j][:-1][::-1]])]]\n",
    "                    else:\n",
    "                        refer4bleu = [[' '.join([self.dp.Y_id2w.get(w, u'&') for w in self.dp.Y_test[i*batch_size+j][:-1]])]]\n",
    "                    candi = [' '.join(w for w in o[j])]\n",
    "                    score = BLEU(candi, refer4bleu, gram=gram)\n",
    "                    all_score.append(score)\n",
    "            #print 'bleu_cost_time:%.3f' % (time.time() -tic)\n",
    "        return np.mean(all_score)\n",
    "    \n",
    "    def show_res(self, path):\n",
    "        res = cPickle.load(open(path))\n",
    "        plt.figure(1)\n",
    "        plt.title('The results') \n",
    "        l1, = plt.plot(res[0], 'g')\n",
    "        l2, = plt.plot(res[1], 'r')\n",
    "        l3, = plt.plot(res[3], 'b')\n",
    "        plt.legend(handles = [l1, l2, l3], labels = [\"Train_loss\",\"Test_loss\",\"BLEU\"], loc = 'best')\n",
    "        plt.show()\n",
    "        \n",
    "    def test_all(self, path, epoch_range, is_bleu=True):\n",
    "        val_loss_list = []\n",
    "        bleu_list = []\n",
    "        for i in range(epoch_range[0], epoch_range[-1]):\n",
    "            self.model.restore(path + str(i))\n",
    "            val_loss = self.test()\n",
    "            val_loss_list.append(val_loss)\n",
    "            if is_bleu:\n",
    "                bleu_score = self.test_bleu()\n",
    "                bleu_list.append(bleu_score)\n",
    "        plt.figure(1)\n",
    "        plt.title('The results') \n",
    "        l1, = plt.plot(val_loss_list,'r')\n",
    "        l2, = plt.plot(bleu_list,'b')\n",
    "        plt.legend(handles = [l1, l2], labels = [\"Test_loss\",\"BLEU\"], loc = 'best')\n",
    "        plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config2name(rnn_size, n_layers, cell_type, residual, reverse=False, emb=False, emb_fix=False):\n",
    "    name = 'rnn_size-%d-n_layers-%d-cell-%s' % (rnn_size, n_layers, cell_type)\n",
    "    if residual:\n",
    "        name += '-residual'\n",
    "    if reverse:\n",
    "        name += '-reverse'\n",
    "    if emb_fix:\n",
    "        name += '-fix_emb'\n",
    "    elif emb:\n",
    "        name += '-emb'\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-13T01:05:36.695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4451, 1024)\n"
     ]
    }
   ],
   "source": [
    "train_dir ='a2c_model/'\n",
    "X_indices, Y_indices = cPickle.load(open('data/a2c_X_Y_indices_no_unk.pkl','rb'))\n",
    "X_w2id, Y_w2id, X_id2w, Y_id2w = cPickle.load(open('data/a2c_Xw2id_Yw2id_Xid2w_Yid2w.pkl','rb'))\n",
    "X_indices = [x[:-1] for x in X_indices]\n",
    "Y_indices = [y[:-1][::-1]+[y[-1],] for y in Y_indices]\n",
    "encoder_emb = cPickle.load(open('data/pre_embedding.pkl'))\n",
    "print encoder_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，了灭覆国六<EOS>\n",
      "；了一统下天，了灭覆国六<EOS>\n",
      "。起耸然巍殿宫房阿<EOS>\n",
      "，伸延折曲西向后然<EOS>\n",
      "。阳咸到通直一，伸延折曲西向后然<EOS>\n",
      "。墙宫了进流<EOS>\n",
      "，楼高座一步五<EOS>\n",
      "，连勾心中拱木的叠层，抱环差参势地的同不借凭自各<EOS>\n",
      "。下上争互在像<EOS>\n",
      "，着旋盘<EOS>\n"
     ]
    }
   ],
   "source": [
    "for y in Y_indices[:10]:\n",
    "    print \"\".join([Y_id2w[idx] for idx in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-12-13T01:05:36.696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data: 323805 | Test_data: 35978 | Batch_size: 128 | Num_batch: 2529 | X_vocab_size: 4451 | Y_vocab_size: 4408\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCH = 10\n",
    "for cell_type in ['lstm']:\n",
    "    for rnn_size in [1024]:\n",
    "        for n_layers in [1]:\n",
    "            for residual in [True]:\n",
    "                dp = Seq2Seq_DP(X_indices, Y_indices, X_w2id, Y_w2id, BATCH_SIZE, n_epoch=NUM_EPOCH)\n",
    "                g = tf.Graph() \n",
    "                sess = tf.Session(graph=g) \n",
    "                with sess.as_default():\n",
    "                    with sess.graph.as_default():\n",
    "                        model = Seq2Seq(\n",
    "                            dp = dp,\n",
    "                            rnn_size = rnn_size,\n",
    "                            n_layers = n_layers,\n",
    "                            encoder_embedding_dim = rnn_size,\n",
    "                            encoder_pre_embedding = encoder_emb,\n",
    "                            decoder_embedding_dim = rnn_size,\n",
    "                            cell_type = cell_type,\n",
    "                            max_infer_length=30,\n",
    "                            residual = residual,\n",
    "                            reverse = True,\n",
    "                            emb_fix = True,\n",
    "                            is_save = True,\n",
    "                            sess= sess\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Seq2Seq_util(dp=dp, model=model, summary_path=config2name(rnn_size, n_layers, cell_type, residual, reverse=True, emb=True, emb_fix=True))\n",
    "#util.fit(train_dir=train_dir, is_bleu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from a2c_model/rnn_size-1024-n_layers-1-cell-lstm-residual-reverse-emb/model-6\n",
      "restore a2c_model/rnn_size-1024-n_layers-1-cell-lstm-residual-reverse-emb/model-6 success\n"
     ]
    }
   ],
   "source": [
    "BLEU_list1 = []\n",
    "for i in range(6, 11):\n",
    "    model.restore('a2c_model/rnn_size-1024-n_layers-1-cell-lstm-residual-reverse-emb/model-%d' % i)\n",
    "    s = util.test_bleu(batch_size=256)\n",
    "    BLEU_list1.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU_list2 = []\n",
    "for i in range(6, 11):\n",
    "    model.restore('a2c_model/rnn_size-1024-n_layers-1-cell-lstm-residual-reverse-fix_emb/model-%d' % i)\n",
    "    s = util.test_bleu(batch_size=256)\n",
    "    BLEU_list2.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print BLEU_list1\n",
    "print BLEU_list2\n",
    "plt.figure()\n",
    "plt.plot(BLEU_list1, 'r--')\n",
    "plt.plot(BLEU_list2, 'b--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.restore('a2c_model/rnn_size-1024-n_layers-1-cell-lstm-residual-reverse-10')\n",
    "s = util.test_bleu(batch_size=256)\n",
    "print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.restore('rnn_size-1024-n_layers-1-cell-lstm-residual-reverse-10')\n",
    "util.cal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = model.infer(u'不为则死！')[0]\n",
    "print Y\n",
    "summary_writer = tf.summary.FileWriter('visual_test/', model.sess.graph)\n",
    "model.add_visual_summary()\n",
    "att = model.write_attention_summary(summary_writer, u'不为一则死！', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    model.write_attention_summary(summary_writer, u'不为则死！', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(X_indices[-10:]):\n",
    "    X = \"\".join([dp.X_id2w[idx] for idx in x])\n",
    "    Y = model.infer(X)[0]\n",
    "    model.show_attention(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
